{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import random\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import itertools\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from itertools import combinations\n",
    "import itertools"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Get the difference\n",
    "# ====================================================\n",
    "def get_difference(data, num_features):\n",
    "    df1 = []\n",
    "    customer_ids = []\n",
    "    for customer_id, df in tqdm(data.groupby(['customer_ID'])):\n",
    "        # Get the differences\n",
    "        diff_df1 = df[num_features].diff(1).iloc[[-1]].values.astype(np.float32)\n",
    "        # Append to lists\n",
    "        df1.append(diff_df1)\n",
    "        customer_ids.append(customer_id)\n",
    "    # Concatenate\n",
    "    df1 = np.concatenate(df1, axis = 0)\n",
    "    # Transform to dataframe\n",
    "    df1 = pd.DataFrame(df1, columns = [col + '_diff1' for col in df[num_features].columns])\n",
    "    # Add customer id\n",
    "    df1['customer_ID'] = customer_ids\n",
    "    return df1\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "\"train = pd.read_parquet('../input/train.parquet')\\ntest = pd.read_parquet('../input/test.parquet')\""
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''train = pd.read_parquet('../input/train.parquet')\n",
    "test = pd.read_parquet('../input/test.parquet')'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "'# Delete columns with missing values above threshold:\\ndef feature_filter(df, threshold=0.95):\\n    filtered_features = [col for col in df.columns if df[col].isnull().sum()/len(df) < threshold]\\n    return filtered_features'"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Delete columns with missing values above threshold:\n",
    "def feature_filter(df, threshold=0.95):\n",
    "    filtered_features = [col for col in df.columns if df[col].isnull().sum()/len(df) < threshold]\n",
    "    return filtered_features'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "'train = train[feature_filter(train)]\\ntest = test[feature_filter(test)]'"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''train = train[feature_filter(train)]\n",
    "test = test[feature_filter(test)]'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "'# Replace null values with appropriate entity:\\ntrain[\"P_2\"].fillna(1.05, inplace = True)\\ntrain[\"P_3\"].fillna(2.45, inplace = True)'"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Replace null values with appropriate entity:\n",
    "train[\"P_2\"].fillna(1.05, inplace = True)\n",
    "train[\"P_3\"].fillna(2.45, inplace = True)'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "\"# Handpicked feature Engineering:\\ndef handpicked(df):\\n    df['payment_default'] = df['P_2'] * df['D_39']\\n    df['payment'] = df.P_2 * df.P_3 * df.P_4\\n    temp = df['D_39'].value_counts().to_dict()\\n    df['D_39_counts'] = df['D_39'].map(temp)\\n    return df\\nhandpicked(train)\\nhandpicked(test)\\nprint(len(train.columns), len(test.columns))\""
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Handpicked feature Engineering:\n",
    "def handpicked(df):\n",
    "    df['payment_default'] = df['P_2'] * df['D_39']\n",
    "    df['payment'] = df.P_2 * df.P_3 * df.P_4\n",
    "    temp = df['D_39'].value_counts().to_dict()\n",
    "    df['D_39_counts'] = df['D_39'].map(temp)\n",
    "    return df\n",
    "handpicked(train)\n",
    "handpicked(test)\n",
    "print(len(train.columns), len(test.columns))'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "\"#compute after-pay feature:\\ndef after_pay(df):\\n    for bcol in [f'B_{i}' for i in [11,14,17]]+['D_39','D_131']+[f'S_{i}' for i in [16,23]]:\\n        for pcol in ['P_2','P_3']:\\n            if bcol in train.columns:\\n                df[f'{bcol}-{pcol}'] = df[bcol] - df[pcol]\\nafter_pay(train)\\nafter_pay(test)\\nprint(len(train.columns), len(test.columns))\""
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''#compute after-pay feature:\n",
    "def after_pay(df):\n",
    "    for bcol in [f'B_{i}' for i in [11,14,17]]+['D_39','D_131']+[f'S_{i}' for i in [16,23]]:\n",
    "        for pcol in ['P_2','P_3']:\n",
    "            if bcol in train.columns:\n",
    "                df[f'{bcol}-{pcol}'] = df[bcol] - df[pcol]\n",
    "after_pay(train)\n",
    "after_pay(test)\n",
    "print(len(train.columns), len(test.columns))'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "\"train.to_parquet('../input/train_smn_2.parquet')\\ntest.to_parquet('../input/test_smn_2.parquet')\""
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''train.to_parquet('../input/train_smn_2.parquet')\n",
    "test.to_parquet('../input/test_smn_2.parquet')'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Read & preprocess data and save it to disk\n",
    "# ====================================================\n",
    "def read_preprocess_data():\n",
    "    train = pd.read_parquet('../input/train_smn_2.parquet')\n",
    "    features = train.drop(['customer_ID', 'S_2'], axis = 1).columns.to_list()\n",
    "    cat_features = [\n",
    "        \"B_30\",\n",
    "        \"B_38\",\n",
    "        \"D_114\",\n",
    "        \"D_116\",\n",
    "        \"D_117\",\n",
    "        \"D_120\",\n",
    "        \"D_126\",\n",
    "        \"D_63\",\n",
    "        \"D_64\",\n",
    "        \"D_66\",\n",
    "        \"D_68\",\n",
    "    ]\n",
    "    num_features = [col for col in features if col not in cat_features]\n",
    "    print('Starting training feature engineer...')\n",
    "    train_num_agg = train.groupby(\"customer_ID\")[num_features].agg(['mean', 'std', 'min', 'max', 'last'])\n",
    "    train_num_agg.columns = ['_'.join(x) for x in train_num_agg.columns]\n",
    "    train_num_agg.reset_index(inplace = True)\n",
    "    train_cat_agg = train.groupby(\"customer_ID\")[cat_features].agg(['count', 'last', 'nunique'])\n",
    "    train_cat_agg.columns = ['_'.join(x) for x in train_cat_agg.columns]\n",
    "    train_cat_agg.reset_index(inplace = True)\n",
    "    train_labels = pd.read_csv('../input/train_labels.csv')\n",
    "    # Transform float64 columns to float32\n",
    "    cols = list(train_num_agg.dtypes[train_num_agg.dtypes == 'float64'].index)\n",
    "    for col in tqdm(cols):\n",
    "        train_num_agg[col] = train_num_agg[col].astype(np.float32)\n",
    "    # Transform int64 columns to int32\n",
    "    cols = list(train_cat_agg.dtypes[train_cat_agg.dtypes == 'int64'].index)\n",
    "    for col in tqdm(cols):\n",
    "        train_cat_agg[col] = train_cat_agg[col].astype(np.int32)\n",
    "    # Get the difference\n",
    "    train_diff = get_difference(train, num_features)\n",
    "    train = train_num_agg.merge(train_cat_agg, how = 'inner', on = 'customer_ID').merge(train_diff, how = 'inner', on = 'customer_ID').merge(train_labels, how = 'inner', on = 'customer_ID')\n",
    "    del train_num_agg, train_cat_agg, train_diff\n",
    "    gc.collect()\n",
    "    test = pd.read_parquet('../input/test_smn_2.parquet')\n",
    "    print('Starting test feature engineer...')\n",
    "    test_num_agg = test.groupby(\"customer_ID\")[num_features].agg(['mean', 'std', 'min', 'max', 'last'])\n",
    "    test_num_agg.columns = ['_'.join(x) for x in test_num_agg.columns]\n",
    "    test_num_agg.reset_index(inplace = True)\n",
    "    test_cat_agg = test.groupby(\"customer_ID\")[cat_features].agg(['count', 'last', 'nunique'])\n",
    "    test_cat_agg.columns = ['_'.join(x) for x in test_cat_agg.columns]\n",
    "    test_cat_agg.reset_index(inplace = True)\n",
    "    # Transform float64 columns to float32\n",
    "    cols = list(test_num_agg.dtypes[test_num_agg.dtypes == 'float64'].index)\n",
    "    for col in tqdm(cols):\n",
    "        test_num_agg[col] = test_num_agg[col].astype(np.float32)\n",
    "    # Transform int64 columns to int32\n",
    "    cols = list(test_cat_agg.dtypes[test_cat_agg.dtypes == 'int64'].index)\n",
    "    for col in tqdm(cols):\n",
    "        test_cat_agg[col] = test_cat_agg[col].astype(np.int32)\n",
    "    # Get the difference\n",
    "    test_diff = get_difference(test, num_features)\n",
    "    test = test_num_agg.merge(test_cat_agg, how = 'inner', on = 'customer_ID').merge(test_diff, how = 'inner', on = 'customer_ID')\n",
    "    del test_num_agg, test_cat_agg, test_diff\n",
    "    gc.collect()\n",
    "    # Save files to disk\n",
    "    train.to_parquet('../input/train_fe_smn_2.parquet')\n",
    "    test.to_parquet('../input/test_fe_smn_2.parquet')\n",
    "\n",
    "# Read & Preprocess Data\n",
    "#read_preprocess_data()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Directory settings\n",
    "# ====================================================\n",
    "import os\n",
    "\n",
    "expt_name = \"lgbm_smn_5_fold_6\"\n",
    "OUTPUT_DIR = f'../{expt_name}/'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "\n",
    "# ====================================================\n",
    "# Configurations\n",
    "# ====================================================\n",
    "class CFG:\n",
    "    input_dir = '../input/'\n",
    "    seed = 42\n",
    "    n_folds = 5\n",
    "    target = 'target'\n",
    "    boosting_type = 'dart'\n",
    "    metric = 'binary_logloss'\n",
    "\n",
    "# ====================================================\n",
    "# Seed everything\n",
    "# ====================================================\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Read data\n",
    "# ====================================================\n",
    "train = pd.read_parquet(CFG.input_dir + 'train_fe_smn_2.parquet')\n",
    "test = pd.read_parquet(CFG.input_dir + 'test_fe_smn_2.parquet')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "                                         customer_ID  P_2_mean   P_2_std   P_2_min   P_2_max  P_2_last  D_39_mean  D_39_std  D_39_min  D_39_max  D_39_last  B_1_mean   B_1_std   B_1_min   B_1_max  B_1_last  B_2_mean   B_2_std   B_2_min   B_2_max  B_2_last  R_1_mean   R_1_std   R_1_min   R_1_max  R_1_last  S_3_mean   S_3_std   S_3_min   S_3_max  S_3_last  D_41_mean  D_41_std  D_41_min  D_41_max  D_41_last  B_3_mean   B_3_std   B_3_min   B_3_max  B_3_last  D_42_mean  D_42_std  D_42_min  D_42_max  D_42_last  D_43_mean  D_43_std  D_43_min  D_43_max  D_43_last  D_44_mean  D_44_std  D_44_min  D_44_max  D_44_last   B_4_mean   B_4_std  B_4_min  B_4_max  B_4_last  D_45_mean  D_45_std  D_45_min  D_45_max  D_45_last  B_5_mean   B_5_std   B_5_min   B_5_max  B_5_last  R_2_mean  R_2_std  R_2_min  R_2_max  R_2_last  D_46_mean  D_46_std  D_46_min  D_46_max  D_46_last  D_47_mean  D_47_std  D_47_min  D_47_max  D_47_last  D_48_mean  D_48_std  D_48_min  D_48_max  D_48_last  D_49_mean  D_49_std  D_49_min  \\\n0  0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...  0.933824  0.024194  0.868580  0.960384  0.934745   0.230769  0.832050         0         3          0  0.012007  0.006547  0.001930  0.021655  0.009382  1.005086  0.003222  1.000242  1.009672  1.007647  0.004509  0.003081  0.000263  0.009228  0.006104  0.113215  0.011670  0.098882  0.135021  0.135021        0.0       0.0       0.0       0.0        0.0  0.006456  0.002942  0.000783  0.009866  0.007174        NaN       NaN       NaN       NaN        NaN        NaN       NaN       NaN       NaN        NaN   0.000000   0.00000         0         0          0   2.846154  2.444250        0        6         5   0.725369  0.009515  0.708906  0.740102   0.740102  0.146650  0.047205  0.060492  0.231717  0.231717       0.0      0.0        0        0         0   0.378074  0.085674  0.231009  0.519619   0.420521   0.532874  0.006578  0.521311  0.542119   0.539715   0.240978  0.076875  0.135586  0.403448   0.192376       -1.0       0.0        -1   \n1  00000fd6641609c6ece5454664794f0340ad84dddce9a2...  0.899820  0.022119  0.861109  0.929122  0.880519   7.153846  6.743468         0        19          6  0.025654  0.027756  0.006711  0.109644  0.034684  0.991083  0.051531  0.819772  1.008534  1.004028  0.006246  0.002129  0.001023  0.008996  0.006911  0.120578  0.023824  0.089799  0.165509  0.165509        0.0       0.0       0.0       0.0        0.0  0.005663  0.003354  0.000861  0.012861  0.005068        NaN       NaN       NaN       NaN        NaN   0.144571  0.169598  0.060646  0.525600   0.060646   0.000000   0.00000         0         0          0   0.846154  0.800641        0        3         1   0.256461  0.009261  0.239459  0.267228   0.266275  0.035462  0.043899  0.004075  0.165146  0.027000       0.0      0.0        0        0         0   0.452041  0.013177  0.432424  0.471737   0.438828   0.392433  0.006671  0.382562  0.402878   0.402195   0.048203  0.031312  0.010117  0.105999   0.014696       -1.0       0.0        -1   \n2  00001b22f846c82c51f6e3958ccd81970162bae8b007e8...  0.878454  0.028911  0.797670  0.904482  0.880875   0.000000  0.000000         0         0          0  0.004386  0.002786  0.001472  0.009997  0.004284  0.815677  0.003545  0.810796  0.819987  0.812649  0.006621  0.001919  0.003540  0.009443  0.006450       NaN       NaN       NaN       NaN       NaN        0.0       0.0       0.0       0.0        0.0  0.005493  0.002834  0.000626  0.009383  0.007196        NaN       NaN       NaN       NaN        NaN        NaN       NaN       NaN       NaN        NaN   0.076923   0.27735         0         1          0   2.230769  1.690850        1        7         2   0.236871  0.008896  0.222406  0.251598   0.251598  0.004618  0.003043  0.000215  0.008656  0.001557       0.0      0.0        0        0         0   0.464475  0.060166  0.413028  0.647064   0.433713   0.328617  0.007183  0.318290  0.339566   0.339125   0.092284  0.060616  0.030227  0.255134   0.080370       -1.0       0.0        -1   \n3  000041bdba6ecadd89a52d11886e8eaaec9325906c9723...  0.598969  0.020107  0.567442  0.623392  0.621776   1.538462  3.017045         0         9          0  0.059876  0.080531  0.005910  0.279991  0.012564  0.955264  0.080981  0.812053  1.009999  1.006183  0.005665  0.003473  0.000199  0.009915  0.007829  0.247750  0.095122  0.149216  0.407420  0.287766        0.0       0.0       0.0       0.0        0.0  0.006423  0.003360  0.000053  0.010927  0.009937        NaN       NaN       NaN       NaN        NaN   0.061026  0.041993  0.006633  0.149891   0.046104   0.000000   0.00000         0         0          0   2.230769  2.832956        0        8         0   0.069334  0.008501  0.056394  0.085103   0.085103  0.088374  0.074462  0.000228  0.283781  0.118818       0.0      0.0        0        0         0   0.431905  0.030525  0.384254  0.471676   0.410723   0.403269  0.006355  0.392230  0.414224   0.414224   0.076686  0.063902  0.005276  0.177252   0.013057       -1.0       0.0        -1   \n4  00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8a...  0.891679  0.042325  0.805045  0.940382  0.871900   0.000000  0.000000         0         0          0  0.005941  0.002475  0.000776  0.009806  0.007679  0.814543  0.003143  0.810670  0.819947  0.815746  0.004180  0.002581  0.000336  0.009076  0.001247  0.173102  0.004669  0.166190  0.176403  0.176403        0.0       0.0       0.0       0.0        0.0  0.005088  0.002910  0.000049  0.009686  0.005528        NaN       NaN       NaN       NaN        NaN   0.048778  0.006847  0.037001  0.061963   0.044671   0.000000   0.00000         0         0          0  11.692307  9.384248        3       25        21   0.209150  0.117203  0.063150  0.305305   0.069952  0.004572  0.002297  0.001201  0.007830  0.004855       0.0      0.0        0        0         0   0.474523  0.076167  0.366783  0.694332   0.465525   0.471961  0.007588  0.461473  0.484715   0.480303   0.253697  0.093176  0.137840  0.491528   0.325121       -1.0       0.0        -1   \n\n   D_49_max  D_49_last  B_6_mean   B_6_std   B_6_min   B_6_max  B_6_last  B_7_mean   B_7_std   B_7_min   B_7_max  B_7_last  B_8_mean   B_8_std   B_8_min   B_8_max  B_8_last  D_50_mean  D_50_std  D_50_min  D_50_max  D_50_last  D_51_mean  D_51_std  D_51_min  D_51_max  D_51_last  B_9_mean   B_9_std   B_9_min   B_9_max  B_9_last  R_3_mean   R_3_std  R_3_min  R_3_max  R_3_last  D_52_mean  D_52_std  D_52_min  D_52_max  D_52_last  P_3_mean   P_3_std   P_3_min   P_3_max  P_3_last  B_10_mean  B_10_std  B_10_min  B_10_max  B_10_last  D_53_mean  D_53_std  D_53_min  D_53_max  D_53_last  S_5_mean   S_5_std   S_5_min   S_5_max  S_5_last  B_11_mean  B_11_std  B_11_min  B_11_max  B_11_last  S_6_mean   S_6_std  S_6_min  S_6_max  S_6_last  D_54_mean  D_54_std  D_54_min  D_54_max  D_54_last  R_4_mean  R_4_std  R_4_min  R_4_max  R_4_last  S_7_mean   S_7_std   S_7_min   S_7_max  S_7_last  B_12_mean  B_12_std  B_12_min  B_12_max  B_12_last     S_8_mean     S_8_std  S_8_min  S_8_max  S_8_last  D_55_mean  \\\n0        -1         -1  0.113510  0.047360  0.063902  0.221899  0.149564  0.036624  0.023195  0.001681  0.060502  0.058425  0.000000  0.000000  0.000000  0.000000  0.000000   0.150326  0.002922  0.145179  0.154326   0.153461   2.923077  0.954074         2         4          2  0.006220  0.003180  0.000519  0.009535  0.009535  0.000000  0.000000        0        0         0   0.204972  0.002400  0.200782  0.208214   0.203524  0.680138  0.050671  0.581678  0.741813  0.629392   0.270280  0.181875  0.096219  0.741934   0.326101        NaN       NaN       NaN       NaN        NaN  0.029112  0.014758  0.007165  0.054221  0.034643   0.007230  0.003031  0.002749  0.010260   0.010260  0.000000  0.000000        0        0         0        1.0       0.0       1.0       1.0        1.0       0.0      0.0        0        0         0  0.098374  0.026775  0.074646  0.161345  0.105671   0.125683  0.011772  0.111060  0.148266   0.112294  2510.000000  429.583527     1544     3166      1544   0.224432   \n1        -1         -1  0.202270  0.015915  0.167634  0.226641  0.167634  0.028049  0.013631  0.015836  0.068204  0.028411  0.000000  0.000000  0.000000  0.000000  0.000000        NaN       NaN       NaN       NaN        NaN   1.153846  0.375534         1         2          1  0.010298  0.011024  0.001722  0.045093  0.012926  0.538462  0.518875        0        1         1   0.158313  0.067030  0.103495  0.242366   0.242366  0.566665  0.036880  0.510142  0.619012  0.570898   0.298815  0.003047  0.294000  0.302757   0.297130        NaN       NaN       NaN       NaN        NaN  0.016785  0.017104  0.002045  0.052949  0.043929   0.013792  0.021041  0.000416  0.081246   0.014570  0.000000  0.000000        0        0         0        1.0       0.0       1.0       1.0        1.0       0.0      0.0        0        0         0  0.103002  0.035143  0.072583  0.208516  0.208516   0.025823  0.004665  0.019050  0.032917   0.019050  1286.461548  772.374573        0     2402      1284   0.048069   \n2        -1         -1  0.176674  0.024615  0.129857  0.213943  0.183628  0.034433  0.015459  0.021261  0.079764  0.026981  0.000000  0.000000  0.000000  0.000000  0.000000        NaN       NaN       NaN       NaN        NaN   0.615385  0.506370         0         1          1  0.004730  0.003302  0.000422  0.009521  0.009392  0.000000  0.000000        0        0         0   0.199863  0.002990  0.195188  0.203649   0.202159  0.618191  0.075604  0.381123  0.678706  0.628938   0.273711  0.052875  0.162125  0.302619   0.296313        NaN       NaN       NaN       NaN        NaN  0.005948  0.002943  0.001054  0.008730  0.001824   0.004683  0.002312  0.000111  0.007619   0.005092  1.000000  0.000000        1        1         1        1.0       0.0       1.0       1.0        1.0       0.0      0.0        0        0         0       NaN       NaN       NaN       NaN       NaN   0.011541  0.002969  0.006100  0.015486   0.007158     0.000000    0.000000        0        0         0   0.077362   \n3        -1         -1  0.160625  0.031266  0.079987  0.196887  0.174331  0.062130  0.073590  0.004301  0.252338  0.011969  1.004676  0.001928  1.002021  1.008767  1.005561   0.439581  0.044539  0.341256  0.482535   0.430318   0.076923  0.277350         0         1          1  0.052241  0.053342  0.001702  0.176352  0.020526  0.615385  0.650444        0        2         2   0.199698  0.002130  0.195300  0.203203   0.198356  0.610934  0.090090  0.345100  0.704214  0.672080   0.306553  0.079528  0.192981  0.431901   0.411625   0.004336  0.003589  0.000346   0.00999   0.001379  0.056297  0.044583  0.002999  0.150845  0.022970   0.044294  0.071076  0.000672  0.241378   0.005491  0.000000  0.000000        0        0         0        1.0       0.0       1.0       1.0        1.0       0.0      0.0        0        0         0  0.261497  0.078128  0.152622  0.370595  0.279464   0.048949  0.025280  0.009411  0.077831   0.074835   961.307678  405.585052      528     1511       528   0.061726   \n4        -1         -1  0.075672  0.046857  0.030852  0.195757  0.048857  0.115290  0.070823  0.035662  0.216773  0.159818  0.386868  0.509339  0.000000  1.008826  1.005185   0.093218  0.020103  0.073834  0.136212   0.095238   0.153846  0.375534         0         1          0  0.006685  0.002242  0.002925  0.009847  0.004027  0.153846  0.375534        0        1         0   0.233470  0.028414  0.191802  0.256440   0.253811  0.527254  0.088509  0.254276  0.584359  0.570419   0.100315  0.074579  0.044728  0.260673   0.125195        NaN       NaN       NaN       NaN        NaN  0.005051  0.002665  0.002389  0.009350  0.009350   0.005017  0.003694  0.000714  0.009807   0.001001  0.846154  0.375534        0        1         1        1.0       0.0       1.0       1.0        1.0       0.0      0.0        0        0         0  0.120290  0.008589  0.108082  0.128201  0.122915   0.049640  0.060154  0.005756  0.151135   0.013041   157.076920  383.420013        0     1021         0   0.203298   \n\n   D_55_std  D_55_min  D_55_max  D_55_last  D_56_mean  D_56_std  D_56_min  D_56_max  D_56_last  B_13_mean  B_13_std  B_13_min  B_13_max  B_13_last  R_5_mean  R_5_std  R_5_min  R_5_max  R_5_last  D_58_mean  D_58_std  D_58_min  D_58_max  D_58_last  S_9_mean   S_9_std   S_9_min   S_9_max  S_9_last  B_14_mean  B_14_std  B_14_min  B_14_max  B_14_last  D_59_mean  D_59_std  D_59_min  D_59_max  D_59_last  D_60_mean  D_60_std  D_60_min  D_60_max  D_60_last  D_61_mean  D_61_std  D_61_min  D_61_max  D_61_last  B_15_mean  B_15_std  B_15_min  B_15_max  B_15_last  S_11_mean  S_11_std  S_11_min  S_11_max  ...  D_131-P_2_min  D_131-P_2_max  D_131-P_2_last  D_131-P_3_mean  D_131-P_3_std  D_131-P_3_min  D_131-P_3_max  D_131-P_3_last  S_16-P_2_mean  S_16-P_2_std  S_16-P_2_min  S_16-P_2_max  S_16-P_2_last  S_16-P_3_mean  S_16-P_3_std  S_16-P_3_min  S_16-P_3_max  S_16-P_3_last  S_23-P_2_mean  S_23-P_2_std  S_23-P_2_min  S_23-P_2_max  S_23-P_2_last  S_23-P_3_mean  S_23-P_3_std  S_23-P_3_min  S_23-P_3_max  \\\n0  0.068116  0.148284  0.354596   0.187285   0.158571  0.004747  0.152025  0.166636   0.166636   0.100432  0.013723  0.074886  0.120740   0.100107       0.0      0.0        0        0         0   0.064803  0.069456  0.000267  0.158612   0.007174  0.039818  0.026706  0.007397  0.093935  0.007397   0.023142  0.013715  0.009725  0.056653   0.010239   7.769231  0.438529         7         8          8   0.534817  0.392130  0.141639  1.009424   0.258461   0.225847  0.071863  0.121276  0.383477   0.227637   0.026247  0.016911  0.007219  0.063955   0.014553  16.615385  1.660244        15        19  ...      -0.960384      -0.868580       -0.934745       -0.680138       0.050671      -0.741813      -0.581678       -0.629392      -0.928850      0.024609     -0.955508     -0.860152      -0.934719      -0.675165      0.049278     -0.736937     -0.577378      -0.629366      -0.798788      0.023300     -0.820325     -0.737125      -0.802944      -0.545102      0.050167     -0.603106     -0.447213   \n1  0.007596  0.036112  0.060770   0.036112   0.705671  0.018540  0.684371  0.748383   0.748383   0.046753  0.024456  0.008499  0.073904   0.017684       0.0      0.0        0        0         0   0.005146  0.002801  0.000004  0.009756   0.009756  0.033809  0.052705  0.006782  0.127805  0.127805   0.014848  0.014395  0.001797  0.057174   0.018667  15.923077  0.277350        15        16         15   0.326530  0.221335  0.059118  0.857541   0.411989   0.053319  0.030845  0.015966  0.103947   0.048978   0.005560  0.002920  0.000095  0.009642   0.009538  14.230769  3.244324        10        23  ...      -0.929122      -0.861109       -0.880519       -0.566665       0.036880      -0.619012      -0.510142       -0.570898      -0.895608      0.022676     -0.927894     -0.852205      -0.873588      -0.562453      0.036707     -0.617784     -0.507154      -0.563967      -0.764205      0.022943     -0.793682     -0.723884      -0.747654      -0.431051      0.038022     -0.483571     -0.376399   \n2  0.016318  0.057529  0.099230   0.098963   0.208154  0.003188  0.201530  0.211538   0.209386   0.003778  0.002688  0.000427  0.008332   0.001749       0.0      0.0        0        0         0   0.023569  0.037544  0.000726  0.093983   0.002847       NaN       NaN       NaN       NaN       NaN   0.004729  0.003074  0.000684  0.008507   0.006699  15.923077  0.277350        15        16         15   0.004735  0.002602  0.000553  0.008550   0.002820   0.109526  0.061762  0.040357  0.249231   0.137834   0.004716  0.002986  0.000019  0.009969   0.006031  12.000000  0.000000        12        12  ...      -0.904482      -0.797670       -0.880875       -0.618191       0.075604      -0.678706      -0.381123       -0.628938      -0.874368      0.029500     -0.903706     -0.793167      -0.871569      -0.614105      0.075772     -0.671846     -0.376619      -0.619633      -0.743706      0.030058     -0.772993     -0.657581      -0.748183      -0.483443      0.077091     -0.544554     -0.241034   \n3  0.018374  0.021400  0.094076   0.021400   0.564632  0.018147  0.533675  0.580167   0.554483   0.081928  0.041875  0.013755  0.124311   0.055897       0.0      0.0        0        0         0   0.023349  0.034747  0.000053  0.088388   0.009294  0.016887  0.008305  0.005059  0.031257  0.011429   0.033350  0.029768  0.006169  0.103393   0.017101  26.538462  2.025479        24        29         29   0.673302  0.331873  0.081805  1.008510   0.394758   0.066872  0.050442  0.026844  0.171638   0.026844   0.004382  0.003003  0.000218  0.009221   0.002199  12.461538  1.664101        10        14  ...      -0.623392      -0.567442       -0.621776       -0.610934       0.090090      -0.704214      -0.345100       -0.672080      -0.593470      0.020696     -0.622428     -0.561958      -0.615063      -0.605434      0.088259     -0.694389     -0.344773      -0.665367      -0.461909      0.021377     -0.488943     -0.427884      -0.488943      -0.473874      0.090423     -0.569697     -0.208708   \n4  0.041725  0.125503  0.254067   0.254067   0.178482  0.009615  0.163719  0.190924   0.183075   0.004422  0.002974  0.000626  0.008859   0.006051       0.0      0.0        0        0         0   0.318151  0.102317  0.094102  0.392473   0.382744       NaN       NaN       NaN       NaN       NaN   0.004924  0.003445  0.000025  0.009628   0.009469  23.153847  3.715870        18        28         28   0.003476  0.002267  0.000846  0.009551   0.002670   0.356445  0.255848  0.082395  0.715081   0.600739   0.006005  0.002529  0.001513  0.009890   0.005842  12.538462  1.391365        12        17  ...      -0.940382      -0.805045       -0.871900       -0.527254       0.088509      -0.584359      -0.254276       -0.570419      -0.887315      0.043650     -0.937680     -0.795330      -0.864329      -0.522890      0.089991     -0.581657     -0.244561      -0.562848      -0.756159      0.042603     -0.805170     -0.665572      -0.739801      -0.391734      0.089403     -0.444325     -0.114802   \n\n   S_23-P_3_last  B_30_count  B_30_last  B_30_nunique  B_38_count  B_38_last  B_38_nunique  D_114_count  D_114_last  D_114_nunique  D_116_count  D_116_last  D_116_nunique  D_117_count  D_117_last  D_117_nunique  D_120_count  D_120_last  D_120_nunique  D_126_count  D_126_last  D_126_nunique  D_63_count  D_63_last  D_63_nunique  D_64_count  D_64_last  D_64_nunique  D_66_count  D_66_last  D_66_nunique  D_68_count  D_68_last  D_68_nunique  P_2_diff1  D_39_diff1  B_1_diff1  B_2_diff1  R_1_diff1  S_3_diff1  D_41_diff1  B_3_diff1  D_42_diff1  D_43_diff1  D_44_diff1  B_4_diff1  D_45_diff1  B_5_diff1  R_2_diff1  D_46_diff1  D_47_diff1  D_48_diff1  D_49_diff1  B_6_diff1  B_7_diff1  B_8_diff1  D_50_diff1  D_51_diff1  B_9_diff1  R_3_diff1  D_52_diff1  P_3_diff1  B_10_diff1  D_53_diff1  S_5_diff1  B_11_diff1  S_6_diff1  D_54_diff1  R_4_diff1  S_7_diff1  B_12_diff1  S_8_diff1  D_55_diff1  D_56_diff1  B_13_diff1  R_5_diff1  D_58_diff1  S_9_diff1  B_14_diff1  D_59_diff1  D_60_diff1  D_61_diff1  \\\n0      -0.497591          13          0             1          13          2             1           13           1              1           13           0              1           13           5              1           13           0              1           13           2              1          13          0             1          13          0             1          13         -1             1          13          6             1  -0.002604         0.0  -0.010455  -0.000660   0.005497   0.032036         0.0  -0.000280         NaN         NaN         0.0       -1.0    0.000040   0.099092        0.0   -0.005328    0.001440   -0.044856         0.0  -0.000590  -0.002076   0.000000    0.004123         0.0   0.008471        0.0   -0.004036  -0.017960    0.000637         NaN  -0.015503    0.003001        0.0         0.0        0.0   0.028320    0.001234     -836.0   -0.009234    0.000188    0.007926        0.0    0.001454   -0.00622   -0.019715         0.0    0.116822   -0.000491   \n1      -0.438033          13          0             1          13          2             1           13           1              1           13           0              1           13           0              1           13           0              2           13           2              1          13          3             1          13          0             1          13         -1             1          13          6             1   0.001663       -12.0   0.000126  -0.001391  -0.000337   0.006024         0.0  -0.003665         NaN   -0.001382         0.0        0.0    0.002539   0.016845        0.0   -0.031395    0.003588    0.004579         0.0  -0.044039  -0.004767   0.000000         NaN         0.0   0.001192        0.0    0.003206   0.053055   -0.004486         NaN   0.037568   -0.000448        0.0         0.0        0.0   0.072336   -0.003547      288.0   -0.011404    0.052660    0.009185        0.0    0.001132        NaN   -0.004625        -1.0   -0.112783   -0.054044   \n2      -0.496247          13          0             1          13          1             1           13           1              2           13           0              1           13           0              1           13           0              1           13           2              1          13          3             1          13          2             1          13         -1             1          13          6             1   0.014532         0.0  -0.004034  -0.006303  -0.000306        NaN         0.0   0.002823         NaN         NaN         0.0       -2.0    0.006082   0.001342        0.0   -0.053204   -0.000441   -0.059945         0.0   0.005560  -0.015999   0.000000         NaN         0.0   0.008970        0.0    0.006024   0.041822   -0.005711         NaN  -0.002897   -0.002436        0.0         0.0        0.0        NaN    0.001058        0.0   -0.000267   -0.002152   -0.000395        0.0   -0.006358        NaN    0.004024        -1.0   -0.005039    0.060685   \n3      -0.539247          13          0             1          13          2             1           13           1              1           13           0              1           13           7              2           13           0              1           13           2              1          13          3             1          13          0             1          13         -1             1          13          3             3  -0.001615         0.0  -0.002025  -0.003816   0.004060  -0.027694         0.0   0.005133         NaN   -0.008052         0.0        0.0    0.010168   0.002863        0.0   -0.015666    0.004188         NaN         0.0  -0.021213  -0.000396   0.000019   -0.016143         1.0  -0.013831        2.0    0.000415  -0.010977    0.009746   -0.005053  -0.007352    0.000565        0.0         0.0        0.0  -0.026107   -0.000784        0.0   -0.021855    0.017433   -0.004979        0.0    0.002537    0.00637    0.001067         0.0   -0.490735   -0.062369   \n4      -0.438320          13          0             1          13          1             2           13           1              1           13           0              1           13           5              1           13           0              1           13           2              1          13          3             1          13          0             1          13          1             1          13          6             1  -0.007338         0.0  -0.000843   0.002465  -0.004237        NaN         0.0   0.001567         NaN    0.007670         0.0       -1.0   -0.001607  -0.000876        0.0    0.009716   -0.004412    0.081822         0.0  -0.005520  -0.004206   0.004426   -0.001273         0.0  -0.005526        0.0   -0.002511   0.000418    0.000639         NaN   0.000884   -0.008344        0.0         0.0        0.0        NaN    0.003644        0.0    0.022100   -0.005202   -0.001676        0.0    0.002218        NaN    0.002199         1.0   -0.001635   -0.026399   \n\n   B_15_diff1  S_11_diff1  D_62_diff1  D_65_diff1  B_16_diff1  B_17_diff1  B_18_diff1  B_19_diff1  B_20_diff1  S_12_diff1  R_6_diff1  S_13_diff1  B_21_diff1  D_69_diff1  B_22_diff1  D_70_diff1  D_71_diff1  D_72_diff1  S_15_diff1  B_23_diff1  P_4_diff1  D_74_diff1  D_75_diff1  D_76_diff1  B_24_diff1  R_7_diff1  D_77_diff1  B_25_diff1  B_26_diff1  D_78_diff1  D_79_diff1  R_8_diff1  R_9_diff1  S_16_diff1  D_80_diff1  R_10_diff1  R_11_diff1  B_27_diff1  D_81_diff1  D_82_diff1  S_17_diff1  R_12_diff1  B_28_diff1  R_13_diff1  D_83_diff1  R_14_diff1  R_15_diff1  D_84_diff1  R_16_diff1  B_29_diff1  S_18_diff1  D_86_diff1  D_87_diff1  R_17_diff1  R_18_diff1  B_31_diff1  S_19_diff1  R_19_diff1  B_32_diff1  S_20_diff1  R_20_diff1  R_21_diff1  B_33_diff1  D_89_diff1  R_22_diff1  R_23_diff1  D_91_diff1  D_92_diff1  D_93_diff1  D_94_diff1  R_24_diff1  R_25_diff1  D_96_diff1  S_22_diff1  S_23_diff1  S_24_diff1  S_25_diff1  S_26_diff1  D_102_diff1  D_103_diff1  D_104_diff1  D_105_diff1  D_106_diff1  \\\n0   -0.027712        -4.0   -0.007366         0.0         0.0         NaN    0.000000         0.0         0.0   -0.003377  -0.000863      -170.0    0.002029   -0.001965         0.0         0.0    0.065053         0.0         1.0   -0.003948        0.0         0.0         0.0         NaN   -0.006484        0.0    0.004078   -0.003098    0.001055         0.0         0.0        0.0        0.0   -0.005000         0.0         0.0         0.0    0.004499         0.0         0.0   -0.000540         0.0   -0.014099         0.0         0.0         0.0         0.0         0.0         0.0         NaN         0.0         0.0         0.0         0.0         0.0         0.0    0.002642         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0   -0.015536   -0.004521   -0.014052   -0.007142   -0.001815     0.020808          0.0    -0.000908     0.006881          0.0   \n1    0.005553        -2.0   -0.004542         0.0         0.0         NaN    0.000000         0.0         0.0    0.003950   0.005626         0.0   -0.002191    0.001692         0.0         0.0   -0.002879         0.0        -1.0   -0.000241        0.0         0.0         0.0         NaN   -0.006784        0.0    0.003503   -0.011997   -0.007419         0.0         0.0        0.0        0.0    0.006864         0.0         0.0         0.0    0.005359         0.0         0.0    0.001794         0.0    0.004686         0.0         0.0         0.0         0.0         0.0         0.0         NaN         0.0         0.0         0.0         0.0         0.0         0.0   -0.004191         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0   -0.073286   -0.007379   -0.080668    0.001961    0.001206     0.000189          0.0     0.000000          NaN          0.0   \n2    0.002519         0.0   -0.002247         0.0         0.0         NaN    0.000000         0.0         0.0    0.003069   0.007775         0.0   -0.006896    0.008785         0.0         0.0    0.001042         0.0         0.0   -0.007777        0.0         0.0         0.0         NaN   -0.002833        0.0    0.005400    0.006263    0.006181         0.0         0.0        0.0        0.0    0.007835         0.0         0.0         0.0   -0.000455         0.0         0.0   -0.006686         0.0   -0.020807         0.0         0.0         0.0         0.0         0.0         0.0         NaN         0.0         0.0         0.0         0.0         0.0         0.0   -0.002523         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0    0.000909   -0.001670    0.000263   -0.001102   -0.002282     0.005371          0.0     0.000000          NaN          0.0   \n3   -0.000796         0.0   -0.000617         0.0        -1.0    0.318869    0.000000         0.0         0.0   -0.295977  -0.003092         0.0   -0.004697   -0.000140         0.0         0.0   -0.001836         0.0         0.0    0.001609        0.0         0.0         0.0         NaN    0.002884        0.0   -0.003236   -0.022100    0.001808         0.0         0.0        0.0        0.0    0.003583         0.0         0.0         1.0    0.007059         0.0         0.0    0.003391         0.0   -0.006715         0.0         0.0         0.0         0.0         0.0         1.0         NaN         0.0         0.0         0.0         0.0         0.0         0.0   -0.004253         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0         1.0         0.0         0.0         0.0         0.0         0.0         0.0   -0.007152   -0.006135    0.007993   -0.003940    0.040299    -0.001329          0.0     0.000000          NaN          0.0   \n4    0.001009         1.0    0.005869         0.0         0.0         NaN    0.001988         0.0         0.0   -0.006730   0.002950         0.0    0.002785    0.004478         0.0         0.0    0.002538         0.0         0.0   -0.008005        0.0         0.0         0.0         NaN    0.006578        0.0   -0.002954    0.006692    0.003931         0.0         0.0        0.0        0.0    0.004710         1.0         0.0         0.0   -0.003384         0.0         0.0    0.007219         0.0   -0.010899         0.0         0.0         0.0         0.0         0.0         0.0         NaN         0.0         0.0         0.0         0.0         0.0         0.0    0.005786         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0   -0.000005    0.000128   -0.000984   -0.005093   -0.007105     0.010343          0.0    -0.000868     0.005986          0.0   \n\n   D_107_diff1  B_36_diff1  B_37_diff1  R_26_diff1  R_27_diff1  D_108_diff1  D_109_diff1  D_111_diff1  D_112_diff1  B_40_diff1  S_27_diff1  D_113_diff1  D_115_diff1  D_118_diff1  D_119_diff1  D_121_diff1  D_122_diff1  D_123_diff1  D_124_diff1  D_125_diff1  D_127_diff1  D_128_diff1  D_129_diff1  B_41_diff1  D_130_diff1  D_131_diff1  D_132_diff1  D_133_diff1  R_28_diff1  D_135_diff1  D_136_diff1  D_137_diff1  D_138_diff1  D_139_diff1  D_140_diff1  D_141_diff1  D_142_diff1  D_143_diff1  D_144_diff1  D_145_diff1  payment_default_diff1  payment_diff1  D_39_counts_diff1  B_11-P_2_diff1  B_11-P_3_diff1  B_14-P_2_diff1  B_14-P_3_diff1  B_17-P_2_diff1  B_17-P_3_diff1  D_39-P_2_diff1  D_39-P_3_diff1  D_131-P_2_diff1  D_131-P_3_diff1  S_16-P_2_diff1  S_16-P_3_diff1  S_23-P_2_diff1  S_23-P_3_diff1  target  \n0          0.0   -0.001942   -0.010028         0.0    0.002271          0.0          0.0          0.0          0.0   -0.000464   -0.050461          0.0    -0.000946     0.006405     0.009222     0.000738          0.0          0.0          0.0          0.0          0.0    -0.000085          0.0         0.0     0.000000          0.0          NaN     0.006088         0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          NaN          0.0    -0.003376          0.0               0.000000            0.0                0.0        0.005605        0.020961       -0.017110       -0.001755             NaN             NaN        0.002604        0.017960         0.002604         0.017960       -0.002396        0.012959       -0.001917        0.013439       0  \n1          0.0    0.002813    0.002615         0.0   -0.001804          0.0          0.0          0.0          0.0   -0.000006    0.288733          0.0     0.005435     0.007249    -0.007188     0.011845          0.0          0.0          0.0          0.0          0.0    -0.007516          0.0         0.0     0.000000          0.0          NaN    -0.006597         0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          NaN          0.0     0.000641          0.0             -10.536292            0.0            33523.0       -0.002111       -0.053503       -0.006288       -0.057681             NaN             NaN      -12.001662      -12.053055        -0.001663        -0.053055        0.005201       -0.046191       -0.009042       -0.060434       0  \n2          0.0   -0.000963   -0.005231         0.0   -0.001479          0.0          0.0          0.0          0.0   -0.014236         NaN          0.0    -0.000573     0.007793     0.009068     0.001451          0.0          0.0          0.0          0.0          0.0     0.000000          0.0         0.0     0.000000          0.0          NaN     0.007902         0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          NaN          0.0    -0.006491          0.0               0.000000            0.0                0.0       -0.016968       -0.044258       -0.010508       -0.037797             NaN             NaN       -0.014532       -0.041822        -0.014532        -0.041822       -0.006697       -0.033986       -0.016202       -0.043491       0  \n3          0.0   -0.003921   -0.005150         0.0   -0.003183          0.0          0.0          0.0          0.0    0.025346    0.058123          0.0     0.006993    -0.001055     0.005564     0.000747          0.0          0.0          0.0          0.0          0.0    -0.001377          0.0         0.0     0.000000          0.0          NaN     0.000632         0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          NaN          0.0     0.000741          0.0               0.000000            0.0                0.0        0.002180        0.011542        0.002683        0.012044        0.320485        0.329846        0.001615        0.010977         0.001615         0.010977        0.005198        0.014560       -0.004520        0.004842       0  \n4          0.0   -0.007248    0.002984         0.0    0.001717          0.0          0.0          0.0          0.0   -0.014405         NaN          0.0     0.003443     0.005265     0.004413     0.006381          0.0          0.0          0.0          0.0          0.0     0.001335          0.0         0.0    -0.002185          0.0          NaN     0.002409         0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          NaN          0.0     0.000618          0.0               0.000000            0.0                0.0       -0.001006       -0.008761        0.009537        0.001781             NaN             NaN        0.007338       -0.000418         0.007338        -0.000418        0.012047        0.004292        0.007465       -0.000290       0  \n\n[5 rows x 1163 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>customer_ID</th>\n      <th>P_2_mean</th>\n      <th>P_2_std</th>\n      <th>P_2_min</th>\n      <th>P_2_max</th>\n      <th>P_2_last</th>\n      <th>D_39_mean</th>\n      <th>D_39_std</th>\n      <th>D_39_min</th>\n      <th>D_39_max</th>\n      <th>D_39_last</th>\n      <th>B_1_mean</th>\n      <th>B_1_std</th>\n      <th>B_1_min</th>\n      <th>B_1_max</th>\n      <th>B_1_last</th>\n      <th>B_2_mean</th>\n      <th>B_2_std</th>\n      <th>B_2_min</th>\n      <th>B_2_max</th>\n      <th>B_2_last</th>\n      <th>R_1_mean</th>\n      <th>R_1_std</th>\n      <th>R_1_min</th>\n      <th>R_1_max</th>\n      <th>R_1_last</th>\n      <th>S_3_mean</th>\n      <th>S_3_std</th>\n      <th>S_3_min</th>\n      <th>S_3_max</th>\n      <th>S_3_last</th>\n      <th>D_41_mean</th>\n      <th>D_41_std</th>\n      <th>D_41_min</th>\n      <th>D_41_max</th>\n      <th>D_41_last</th>\n      <th>B_3_mean</th>\n      <th>B_3_std</th>\n      <th>B_3_min</th>\n      <th>B_3_max</th>\n      <th>B_3_last</th>\n      <th>D_42_mean</th>\n      <th>D_42_std</th>\n      <th>D_42_min</th>\n      <th>D_42_max</th>\n      <th>D_42_last</th>\n      <th>D_43_mean</th>\n      <th>D_43_std</th>\n      <th>D_43_min</th>\n      <th>D_43_max</th>\n      <th>D_43_last</th>\n      <th>D_44_mean</th>\n      <th>D_44_std</th>\n      <th>D_44_min</th>\n      <th>D_44_max</th>\n      <th>D_44_last</th>\n      <th>B_4_mean</th>\n      <th>B_4_std</th>\n      <th>B_4_min</th>\n      <th>B_4_max</th>\n      <th>B_4_last</th>\n      <th>D_45_mean</th>\n      <th>D_45_std</th>\n      <th>D_45_min</th>\n      <th>D_45_max</th>\n      <th>D_45_last</th>\n      <th>B_5_mean</th>\n      <th>B_5_std</th>\n      <th>B_5_min</th>\n      <th>B_5_max</th>\n      <th>B_5_last</th>\n      <th>R_2_mean</th>\n      <th>R_2_std</th>\n      <th>R_2_min</th>\n      <th>R_2_max</th>\n      <th>R_2_last</th>\n      <th>D_46_mean</th>\n      <th>D_46_std</th>\n      <th>D_46_min</th>\n      <th>D_46_max</th>\n      <th>D_46_last</th>\n      <th>D_47_mean</th>\n      <th>D_47_std</th>\n      <th>D_47_min</th>\n      <th>D_47_max</th>\n      <th>D_47_last</th>\n      <th>D_48_mean</th>\n      <th>D_48_std</th>\n      <th>D_48_min</th>\n      <th>D_48_max</th>\n      <th>D_48_last</th>\n      <th>D_49_mean</th>\n      <th>D_49_std</th>\n      <th>D_49_min</th>\n      <th>D_49_max</th>\n      <th>D_49_last</th>\n      <th>B_6_mean</th>\n      <th>B_6_std</th>\n      <th>B_6_min</th>\n      <th>B_6_max</th>\n      <th>B_6_last</th>\n      <th>B_7_mean</th>\n      <th>B_7_std</th>\n      <th>B_7_min</th>\n      <th>B_7_max</th>\n      <th>B_7_last</th>\n      <th>B_8_mean</th>\n      <th>B_8_std</th>\n      <th>B_8_min</th>\n      <th>B_8_max</th>\n      <th>B_8_last</th>\n      <th>D_50_mean</th>\n      <th>D_50_std</th>\n      <th>D_50_min</th>\n      <th>D_50_max</th>\n      <th>D_50_last</th>\n      <th>D_51_mean</th>\n      <th>D_51_std</th>\n      <th>D_51_min</th>\n      <th>D_51_max</th>\n      <th>D_51_last</th>\n      <th>B_9_mean</th>\n      <th>B_9_std</th>\n      <th>B_9_min</th>\n      <th>B_9_max</th>\n      <th>B_9_last</th>\n      <th>R_3_mean</th>\n      <th>R_3_std</th>\n      <th>R_3_min</th>\n      <th>R_3_max</th>\n      <th>R_3_last</th>\n      <th>D_52_mean</th>\n      <th>D_52_std</th>\n      <th>D_52_min</th>\n      <th>D_52_max</th>\n      <th>D_52_last</th>\n      <th>P_3_mean</th>\n      <th>P_3_std</th>\n      <th>P_3_min</th>\n      <th>P_3_max</th>\n      <th>P_3_last</th>\n      <th>B_10_mean</th>\n      <th>B_10_std</th>\n      <th>B_10_min</th>\n      <th>B_10_max</th>\n      <th>B_10_last</th>\n      <th>D_53_mean</th>\n      <th>D_53_std</th>\n      <th>D_53_min</th>\n      <th>D_53_max</th>\n      <th>D_53_last</th>\n      <th>S_5_mean</th>\n      <th>S_5_std</th>\n      <th>S_5_min</th>\n      <th>S_5_max</th>\n      <th>S_5_last</th>\n      <th>B_11_mean</th>\n      <th>B_11_std</th>\n      <th>B_11_min</th>\n      <th>B_11_max</th>\n      <th>B_11_last</th>\n      <th>S_6_mean</th>\n      <th>S_6_std</th>\n      <th>S_6_min</th>\n      <th>S_6_max</th>\n      <th>S_6_last</th>\n      <th>D_54_mean</th>\n      <th>D_54_std</th>\n      <th>D_54_min</th>\n      <th>D_54_max</th>\n      <th>D_54_last</th>\n      <th>R_4_mean</th>\n      <th>R_4_std</th>\n      <th>R_4_min</th>\n      <th>R_4_max</th>\n      <th>R_4_last</th>\n      <th>S_7_mean</th>\n      <th>S_7_std</th>\n      <th>S_7_min</th>\n      <th>S_7_max</th>\n      <th>S_7_last</th>\n      <th>B_12_mean</th>\n      <th>B_12_std</th>\n      <th>B_12_min</th>\n      <th>B_12_max</th>\n      <th>B_12_last</th>\n      <th>S_8_mean</th>\n      <th>S_8_std</th>\n      <th>S_8_min</th>\n      <th>S_8_max</th>\n      <th>S_8_last</th>\n      <th>D_55_mean</th>\n      <th>D_55_std</th>\n      <th>D_55_min</th>\n      <th>D_55_max</th>\n      <th>D_55_last</th>\n      <th>D_56_mean</th>\n      <th>D_56_std</th>\n      <th>D_56_min</th>\n      <th>D_56_max</th>\n      <th>D_56_last</th>\n      <th>B_13_mean</th>\n      <th>B_13_std</th>\n      <th>B_13_min</th>\n      <th>B_13_max</th>\n      <th>B_13_last</th>\n      <th>R_5_mean</th>\n      <th>R_5_std</th>\n      <th>R_5_min</th>\n      <th>R_5_max</th>\n      <th>R_5_last</th>\n      <th>D_58_mean</th>\n      <th>D_58_std</th>\n      <th>D_58_min</th>\n      <th>D_58_max</th>\n      <th>D_58_last</th>\n      <th>S_9_mean</th>\n      <th>S_9_std</th>\n      <th>S_9_min</th>\n      <th>S_9_max</th>\n      <th>S_9_last</th>\n      <th>B_14_mean</th>\n      <th>B_14_std</th>\n      <th>B_14_min</th>\n      <th>B_14_max</th>\n      <th>B_14_last</th>\n      <th>D_59_mean</th>\n      <th>D_59_std</th>\n      <th>D_59_min</th>\n      <th>D_59_max</th>\n      <th>D_59_last</th>\n      <th>D_60_mean</th>\n      <th>D_60_std</th>\n      <th>D_60_min</th>\n      <th>D_60_max</th>\n      <th>D_60_last</th>\n      <th>D_61_mean</th>\n      <th>D_61_std</th>\n      <th>D_61_min</th>\n      <th>D_61_max</th>\n      <th>D_61_last</th>\n      <th>B_15_mean</th>\n      <th>B_15_std</th>\n      <th>B_15_min</th>\n      <th>B_15_max</th>\n      <th>B_15_last</th>\n      <th>S_11_mean</th>\n      <th>S_11_std</th>\n      <th>S_11_min</th>\n      <th>S_11_max</th>\n      <th>...</th>\n      <th>D_131-P_2_min</th>\n      <th>D_131-P_2_max</th>\n      <th>D_131-P_2_last</th>\n      <th>D_131-P_3_mean</th>\n      <th>D_131-P_3_std</th>\n      <th>D_131-P_3_min</th>\n      <th>D_131-P_3_max</th>\n      <th>D_131-P_3_last</th>\n      <th>S_16-P_2_mean</th>\n      <th>S_16-P_2_std</th>\n      <th>S_16-P_2_min</th>\n      <th>S_16-P_2_max</th>\n      <th>S_16-P_2_last</th>\n      <th>S_16-P_3_mean</th>\n      <th>S_16-P_3_std</th>\n      <th>S_16-P_3_min</th>\n      <th>S_16-P_3_max</th>\n      <th>S_16-P_3_last</th>\n      <th>S_23-P_2_mean</th>\n      <th>S_23-P_2_std</th>\n      <th>S_23-P_2_min</th>\n      <th>S_23-P_2_max</th>\n      <th>S_23-P_2_last</th>\n      <th>S_23-P_3_mean</th>\n      <th>S_23-P_3_std</th>\n      <th>S_23-P_3_min</th>\n      <th>S_23-P_3_max</th>\n      <th>S_23-P_3_last</th>\n      <th>B_30_count</th>\n      <th>B_30_last</th>\n      <th>B_30_nunique</th>\n      <th>B_38_count</th>\n      <th>B_38_last</th>\n      <th>B_38_nunique</th>\n      <th>D_114_count</th>\n      <th>D_114_last</th>\n      <th>D_114_nunique</th>\n      <th>D_116_count</th>\n      <th>D_116_last</th>\n      <th>D_116_nunique</th>\n      <th>D_117_count</th>\n      <th>D_117_last</th>\n      <th>D_117_nunique</th>\n      <th>D_120_count</th>\n      <th>D_120_last</th>\n      <th>D_120_nunique</th>\n      <th>D_126_count</th>\n      <th>D_126_last</th>\n      <th>D_126_nunique</th>\n      <th>D_63_count</th>\n      <th>D_63_last</th>\n      <th>D_63_nunique</th>\n      <th>D_64_count</th>\n      <th>D_64_last</th>\n      <th>D_64_nunique</th>\n      <th>D_66_count</th>\n      <th>D_66_last</th>\n      <th>D_66_nunique</th>\n      <th>D_68_count</th>\n      <th>D_68_last</th>\n      <th>D_68_nunique</th>\n      <th>P_2_diff1</th>\n      <th>D_39_diff1</th>\n      <th>B_1_diff1</th>\n      <th>B_2_diff1</th>\n      <th>R_1_diff1</th>\n      <th>S_3_diff1</th>\n      <th>D_41_diff1</th>\n      <th>B_3_diff1</th>\n      <th>D_42_diff1</th>\n      <th>D_43_diff1</th>\n      <th>D_44_diff1</th>\n      <th>B_4_diff1</th>\n      <th>D_45_diff1</th>\n      <th>B_5_diff1</th>\n      <th>R_2_diff1</th>\n      <th>D_46_diff1</th>\n      <th>D_47_diff1</th>\n      <th>D_48_diff1</th>\n      <th>D_49_diff1</th>\n      <th>B_6_diff1</th>\n      <th>B_7_diff1</th>\n      <th>B_8_diff1</th>\n      <th>D_50_diff1</th>\n      <th>D_51_diff1</th>\n      <th>B_9_diff1</th>\n      <th>R_3_diff1</th>\n      <th>D_52_diff1</th>\n      <th>P_3_diff1</th>\n      <th>B_10_diff1</th>\n      <th>D_53_diff1</th>\n      <th>S_5_diff1</th>\n      <th>B_11_diff1</th>\n      <th>S_6_diff1</th>\n      <th>D_54_diff1</th>\n      <th>R_4_diff1</th>\n      <th>S_7_diff1</th>\n      <th>B_12_diff1</th>\n      <th>S_8_diff1</th>\n      <th>D_55_diff1</th>\n      <th>D_56_diff1</th>\n      <th>B_13_diff1</th>\n      <th>R_5_diff1</th>\n      <th>D_58_diff1</th>\n      <th>S_9_diff1</th>\n      <th>B_14_diff1</th>\n      <th>D_59_diff1</th>\n      <th>D_60_diff1</th>\n      <th>D_61_diff1</th>\n      <th>B_15_diff1</th>\n      <th>S_11_diff1</th>\n      <th>D_62_diff1</th>\n      <th>D_65_diff1</th>\n      <th>B_16_diff1</th>\n      <th>B_17_diff1</th>\n      <th>B_18_diff1</th>\n      <th>B_19_diff1</th>\n      <th>B_20_diff1</th>\n      <th>S_12_diff1</th>\n      <th>R_6_diff1</th>\n      <th>S_13_diff1</th>\n      <th>B_21_diff1</th>\n      <th>D_69_diff1</th>\n      <th>B_22_diff1</th>\n      <th>D_70_diff1</th>\n      <th>D_71_diff1</th>\n      <th>D_72_diff1</th>\n      <th>S_15_diff1</th>\n      <th>B_23_diff1</th>\n      <th>P_4_diff1</th>\n      <th>D_74_diff1</th>\n      <th>D_75_diff1</th>\n      <th>D_76_diff1</th>\n      <th>B_24_diff1</th>\n      <th>R_7_diff1</th>\n      <th>D_77_diff1</th>\n      <th>B_25_diff1</th>\n      <th>B_26_diff1</th>\n      <th>D_78_diff1</th>\n      <th>D_79_diff1</th>\n      <th>R_8_diff1</th>\n      <th>R_9_diff1</th>\n      <th>S_16_diff1</th>\n      <th>D_80_diff1</th>\n      <th>R_10_diff1</th>\n      <th>R_11_diff1</th>\n      <th>B_27_diff1</th>\n      <th>D_81_diff1</th>\n      <th>D_82_diff1</th>\n      <th>S_17_diff1</th>\n      <th>R_12_diff1</th>\n      <th>B_28_diff1</th>\n      <th>R_13_diff1</th>\n      <th>D_83_diff1</th>\n      <th>R_14_diff1</th>\n      <th>R_15_diff1</th>\n      <th>D_84_diff1</th>\n      <th>R_16_diff1</th>\n      <th>B_29_diff1</th>\n      <th>S_18_diff1</th>\n      <th>D_86_diff1</th>\n      <th>D_87_diff1</th>\n      <th>R_17_diff1</th>\n      <th>R_18_diff1</th>\n      <th>B_31_diff1</th>\n      <th>S_19_diff1</th>\n      <th>R_19_diff1</th>\n      <th>B_32_diff1</th>\n      <th>S_20_diff1</th>\n      <th>R_20_diff1</th>\n      <th>R_21_diff1</th>\n      <th>B_33_diff1</th>\n      <th>D_89_diff1</th>\n      <th>R_22_diff1</th>\n      <th>R_23_diff1</th>\n      <th>D_91_diff1</th>\n      <th>D_92_diff1</th>\n      <th>D_93_diff1</th>\n      <th>D_94_diff1</th>\n      <th>R_24_diff1</th>\n      <th>R_25_diff1</th>\n      <th>D_96_diff1</th>\n      <th>S_22_diff1</th>\n      <th>S_23_diff1</th>\n      <th>S_24_diff1</th>\n      <th>S_25_diff1</th>\n      <th>S_26_diff1</th>\n      <th>D_102_diff1</th>\n      <th>D_103_diff1</th>\n      <th>D_104_diff1</th>\n      <th>D_105_diff1</th>\n      <th>D_106_diff1</th>\n      <th>D_107_diff1</th>\n      <th>B_36_diff1</th>\n      <th>B_37_diff1</th>\n      <th>R_26_diff1</th>\n      <th>R_27_diff1</th>\n      <th>D_108_diff1</th>\n      <th>D_109_diff1</th>\n      <th>D_111_diff1</th>\n      <th>D_112_diff1</th>\n      <th>B_40_diff1</th>\n      <th>S_27_diff1</th>\n      <th>D_113_diff1</th>\n      <th>D_115_diff1</th>\n      <th>D_118_diff1</th>\n      <th>D_119_diff1</th>\n      <th>D_121_diff1</th>\n      <th>D_122_diff1</th>\n      <th>D_123_diff1</th>\n      <th>D_124_diff1</th>\n      <th>D_125_diff1</th>\n      <th>D_127_diff1</th>\n      <th>D_128_diff1</th>\n      <th>D_129_diff1</th>\n      <th>B_41_diff1</th>\n      <th>D_130_diff1</th>\n      <th>D_131_diff1</th>\n      <th>D_132_diff1</th>\n      <th>D_133_diff1</th>\n      <th>R_28_diff1</th>\n      <th>D_135_diff1</th>\n      <th>D_136_diff1</th>\n      <th>D_137_diff1</th>\n      <th>D_138_diff1</th>\n      <th>D_139_diff1</th>\n      <th>D_140_diff1</th>\n      <th>D_141_diff1</th>\n      <th>D_142_diff1</th>\n      <th>D_143_diff1</th>\n      <th>D_144_diff1</th>\n      <th>D_145_diff1</th>\n      <th>payment_default_diff1</th>\n      <th>payment_diff1</th>\n      <th>D_39_counts_diff1</th>\n      <th>B_11-P_2_diff1</th>\n      <th>B_11-P_3_diff1</th>\n      <th>B_14-P_2_diff1</th>\n      <th>B_14-P_3_diff1</th>\n      <th>B_17-P_2_diff1</th>\n      <th>B_17-P_3_diff1</th>\n      <th>D_39-P_2_diff1</th>\n      <th>D_39-P_3_diff1</th>\n      <th>D_131-P_2_diff1</th>\n      <th>D_131-P_3_diff1</th>\n      <th>S_16-P_2_diff1</th>\n      <th>S_16-P_3_diff1</th>\n      <th>S_23-P_2_diff1</th>\n      <th>S_23-P_3_diff1</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...</td>\n      <td>0.933824</td>\n      <td>0.024194</td>\n      <td>0.868580</td>\n      <td>0.960384</td>\n      <td>0.934745</td>\n      <td>0.230769</td>\n      <td>0.832050</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0.012007</td>\n      <td>0.006547</td>\n      <td>0.001930</td>\n      <td>0.021655</td>\n      <td>0.009382</td>\n      <td>1.005086</td>\n      <td>0.003222</td>\n      <td>1.000242</td>\n      <td>1.009672</td>\n      <td>1.007647</td>\n      <td>0.004509</td>\n      <td>0.003081</td>\n      <td>0.000263</td>\n      <td>0.009228</td>\n      <td>0.006104</td>\n      <td>0.113215</td>\n      <td>0.011670</td>\n      <td>0.098882</td>\n      <td>0.135021</td>\n      <td>0.135021</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.006456</td>\n      <td>0.002942</td>\n      <td>0.000783</td>\n      <td>0.009866</td>\n      <td>0.007174</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2.846154</td>\n      <td>2.444250</td>\n      <td>0</td>\n      <td>6</td>\n      <td>5</td>\n      <td>0.725369</td>\n      <td>0.009515</td>\n      <td>0.708906</td>\n      <td>0.740102</td>\n      <td>0.740102</td>\n      <td>0.146650</td>\n      <td>0.047205</td>\n      <td>0.060492</td>\n      <td>0.231717</td>\n      <td>0.231717</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.378074</td>\n      <td>0.085674</td>\n      <td>0.231009</td>\n      <td>0.519619</td>\n      <td>0.420521</td>\n      <td>0.532874</td>\n      <td>0.006578</td>\n      <td>0.521311</td>\n      <td>0.542119</td>\n      <td>0.539715</td>\n      <td>0.240978</td>\n      <td>0.076875</td>\n      <td>0.135586</td>\n      <td>0.403448</td>\n      <td>0.192376</td>\n      <td>-1.0</td>\n      <td>0.0</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>0.113510</td>\n      <td>0.047360</td>\n      <td>0.063902</td>\n      <td>0.221899</td>\n      <td>0.149564</td>\n      <td>0.036624</td>\n      <td>0.023195</td>\n      <td>0.001681</td>\n      <td>0.060502</td>\n      <td>0.058425</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.150326</td>\n      <td>0.002922</td>\n      <td>0.145179</td>\n      <td>0.154326</td>\n      <td>0.153461</td>\n      <td>2.923077</td>\n      <td>0.954074</td>\n      <td>2</td>\n      <td>4</td>\n      <td>2</td>\n      <td>0.006220</td>\n      <td>0.003180</td>\n      <td>0.000519</td>\n      <td>0.009535</td>\n      <td>0.009535</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.204972</td>\n      <td>0.002400</td>\n      <td>0.200782</td>\n      <td>0.208214</td>\n      <td>0.203524</td>\n      <td>0.680138</td>\n      <td>0.050671</td>\n      <td>0.581678</td>\n      <td>0.741813</td>\n      <td>0.629392</td>\n      <td>0.270280</td>\n      <td>0.181875</td>\n      <td>0.096219</td>\n      <td>0.741934</td>\n      <td>0.326101</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.029112</td>\n      <td>0.014758</td>\n      <td>0.007165</td>\n      <td>0.054221</td>\n      <td>0.034643</td>\n      <td>0.007230</td>\n      <td>0.003031</td>\n      <td>0.002749</td>\n      <td>0.010260</td>\n      <td>0.010260</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.098374</td>\n      <td>0.026775</td>\n      <td>0.074646</td>\n      <td>0.161345</td>\n      <td>0.105671</td>\n      <td>0.125683</td>\n      <td>0.011772</td>\n      <td>0.111060</td>\n      <td>0.148266</td>\n      <td>0.112294</td>\n      <td>2510.000000</td>\n      <td>429.583527</td>\n      <td>1544</td>\n      <td>3166</td>\n      <td>1544</td>\n      <td>0.224432</td>\n      <td>0.068116</td>\n      <td>0.148284</td>\n      <td>0.354596</td>\n      <td>0.187285</td>\n      <td>0.158571</td>\n      <td>0.004747</td>\n      <td>0.152025</td>\n      <td>0.166636</td>\n      <td>0.166636</td>\n      <td>0.100432</td>\n      <td>0.013723</td>\n      <td>0.074886</td>\n      <td>0.120740</td>\n      <td>0.100107</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.064803</td>\n      <td>0.069456</td>\n      <td>0.000267</td>\n      <td>0.158612</td>\n      <td>0.007174</td>\n      <td>0.039818</td>\n      <td>0.026706</td>\n      <td>0.007397</td>\n      <td>0.093935</td>\n      <td>0.007397</td>\n      <td>0.023142</td>\n      <td>0.013715</td>\n      <td>0.009725</td>\n      <td>0.056653</td>\n      <td>0.010239</td>\n      <td>7.769231</td>\n      <td>0.438529</td>\n      <td>7</td>\n      <td>8</td>\n      <td>8</td>\n      <td>0.534817</td>\n      <td>0.392130</td>\n      <td>0.141639</td>\n      <td>1.009424</td>\n      <td>0.258461</td>\n      <td>0.225847</td>\n      <td>0.071863</td>\n      <td>0.121276</td>\n      <td>0.383477</td>\n      <td>0.227637</td>\n      <td>0.026247</td>\n      <td>0.016911</td>\n      <td>0.007219</td>\n      <td>0.063955</td>\n      <td>0.014553</td>\n      <td>16.615385</td>\n      <td>1.660244</td>\n      <td>15</td>\n      <td>19</td>\n      <td>...</td>\n      <td>-0.960384</td>\n      <td>-0.868580</td>\n      <td>-0.934745</td>\n      <td>-0.680138</td>\n      <td>0.050671</td>\n      <td>-0.741813</td>\n      <td>-0.581678</td>\n      <td>-0.629392</td>\n      <td>-0.928850</td>\n      <td>0.024609</td>\n      <td>-0.955508</td>\n      <td>-0.860152</td>\n      <td>-0.934719</td>\n      <td>-0.675165</td>\n      <td>0.049278</td>\n      <td>-0.736937</td>\n      <td>-0.577378</td>\n      <td>-0.629366</td>\n      <td>-0.798788</td>\n      <td>0.023300</td>\n      <td>-0.820325</td>\n      <td>-0.737125</td>\n      <td>-0.802944</td>\n      <td>-0.545102</td>\n      <td>0.050167</td>\n      <td>-0.603106</td>\n      <td>-0.447213</td>\n      <td>-0.497591</td>\n      <td>13</td>\n      <td>0</td>\n      <td>1</td>\n      <td>13</td>\n      <td>2</td>\n      <td>1</td>\n      <td>13</td>\n      <td>1</td>\n      <td>1</td>\n      <td>13</td>\n      <td>0</td>\n      <td>1</td>\n      <td>13</td>\n      <td>5</td>\n      <td>1</td>\n      <td>13</td>\n      <td>0</td>\n      <td>1</td>\n      <td>13</td>\n      <td>2</td>\n      <td>1</td>\n      <td>13</td>\n      <td>0</td>\n      <td>1</td>\n      <td>13</td>\n      <td>0</td>\n      <td>1</td>\n      <td>13</td>\n      <td>-1</td>\n      <td>1</td>\n      <td>13</td>\n      <td>6</td>\n      <td>1</td>\n      <td>-0.002604</td>\n      <td>0.0</td>\n      <td>-0.010455</td>\n      <td>-0.000660</td>\n      <td>0.005497</td>\n      <td>0.032036</td>\n      <td>0.0</td>\n      <td>-0.000280</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>-1.0</td>\n      <td>0.000040</td>\n      <td>0.099092</td>\n      <td>0.0</td>\n      <td>-0.005328</td>\n      <td>0.001440</td>\n      <td>-0.044856</td>\n      <td>0.0</td>\n      <td>-0.000590</td>\n      <td>-0.002076</td>\n      <td>0.000000</td>\n      <td>0.004123</td>\n      <td>0.0</td>\n      <td>0.008471</td>\n      <td>0.0</td>\n      <td>-0.004036</td>\n      <td>-0.017960</td>\n      <td>0.000637</td>\n      <td>NaN</td>\n      <td>-0.015503</td>\n      <td>0.003001</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.028320</td>\n      <td>0.001234</td>\n      <td>-836.0</td>\n      <td>-0.009234</td>\n      <td>0.000188</td>\n      <td>0.007926</td>\n      <td>0.0</td>\n      <td>0.001454</td>\n      <td>-0.00622</td>\n      <td>-0.019715</td>\n      <td>0.0</td>\n      <td>0.116822</td>\n      <td>-0.000491</td>\n      <td>-0.027712</td>\n      <td>-4.0</td>\n      <td>-0.007366</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.003377</td>\n      <td>-0.000863</td>\n      <td>-170.0</td>\n      <td>0.002029</td>\n      <td>-0.001965</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.065053</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>-0.003948</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>-0.006484</td>\n      <td>0.0</td>\n      <td>0.004078</td>\n      <td>-0.003098</td>\n      <td>0.001055</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.005000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.004499</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.000540</td>\n      <td>0.0</td>\n      <td>-0.014099</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.002642</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.015536</td>\n      <td>-0.004521</td>\n      <td>-0.014052</td>\n      <td>-0.007142</td>\n      <td>-0.001815</td>\n      <td>0.020808</td>\n      <td>0.0</td>\n      <td>-0.000908</td>\n      <td>0.006881</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.001942</td>\n      <td>-0.010028</td>\n      <td>0.0</td>\n      <td>0.002271</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.000464</td>\n      <td>-0.050461</td>\n      <td>0.0</td>\n      <td>-0.000946</td>\n      <td>0.006405</td>\n      <td>0.009222</td>\n      <td>0.000738</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.000085</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0.006088</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>-0.003376</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.005605</td>\n      <td>0.020961</td>\n      <td>-0.017110</td>\n      <td>-0.001755</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.002604</td>\n      <td>0.017960</td>\n      <td>0.002604</td>\n      <td>0.017960</td>\n      <td>-0.002396</td>\n      <td>0.012959</td>\n      <td>-0.001917</td>\n      <td>0.013439</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>00000fd6641609c6ece5454664794f0340ad84dddce9a2...</td>\n      <td>0.899820</td>\n      <td>0.022119</td>\n      <td>0.861109</td>\n      <td>0.929122</td>\n      <td>0.880519</td>\n      <td>7.153846</td>\n      <td>6.743468</td>\n      <td>0</td>\n      <td>19</td>\n      <td>6</td>\n      <td>0.025654</td>\n      <td>0.027756</td>\n      <td>0.006711</td>\n      <td>0.109644</td>\n      <td>0.034684</td>\n      <td>0.991083</td>\n      <td>0.051531</td>\n      <td>0.819772</td>\n      <td>1.008534</td>\n      <td>1.004028</td>\n      <td>0.006246</td>\n      <td>0.002129</td>\n      <td>0.001023</td>\n      <td>0.008996</td>\n      <td>0.006911</td>\n      <td>0.120578</td>\n      <td>0.023824</td>\n      <td>0.089799</td>\n      <td>0.165509</td>\n      <td>0.165509</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.005663</td>\n      <td>0.003354</td>\n      <td>0.000861</td>\n      <td>0.012861</td>\n      <td>0.005068</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.144571</td>\n      <td>0.169598</td>\n      <td>0.060646</td>\n      <td>0.525600</td>\n      <td>0.060646</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.846154</td>\n      <td>0.800641</td>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0.256461</td>\n      <td>0.009261</td>\n      <td>0.239459</td>\n      <td>0.267228</td>\n      <td>0.266275</td>\n      <td>0.035462</td>\n      <td>0.043899</td>\n      <td>0.004075</td>\n      <td>0.165146</td>\n      <td>0.027000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.452041</td>\n      <td>0.013177</td>\n      <td>0.432424</td>\n      <td>0.471737</td>\n      <td>0.438828</td>\n      <td>0.392433</td>\n      <td>0.006671</td>\n      <td>0.382562</td>\n      <td>0.402878</td>\n      <td>0.402195</td>\n      <td>0.048203</td>\n      <td>0.031312</td>\n      <td>0.010117</td>\n      <td>0.105999</td>\n      <td>0.014696</td>\n      <td>-1.0</td>\n      <td>0.0</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>0.202270</td>\n      <td>0.015915</td>\n      <td>0.167634</td>\n      <td>0.226641</td>\n      <td>0.167634</td>\n      <td>0.028049</td>\n      <td>0.013631</td>\n      <td>0.015836</td>\n      <td>0.068204</td>\n      <td>0.028411</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.153846</td>\n      <td>0.375534</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0.010298</td>\n      <td>0.011024</td>\n      <td>0.001722</td>\n      <td>0.045093</td>\n      <td>0.012926</td>\n      <td>0.538462</td>\n      <td>0.518875</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.158313</td>\n      <td>0.067030</td>\n      <td>0.103495</td>\n      <td>0.242366</td>\n      <td>0.242366</td>\n      <td>0.566665</td>\n      <td>0.036880</td>\n      <td>0.510142</td>\n      <td>0.619012</td>\n      <td>0.570898</td>\n      <td>0.298815</td>\n      <td>0.003047</td>\n      <td>0.294000</td>\n      <td>0.302757</td>\n      <td>0.297130</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.016785</td>\n      <td>0.017104</td>\n      <td>0.002045</td>\n      <td>0.052949</td>\n      <td>0.043929</td>\n      <td>0.013792</td>\n      <td>0.021041</td>\n      <td>0.000416</td>\n      <td>0.081246</td>\n      <td>0.014570</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.103002</td>\n      <td>0.035143</td>\n      <td>0.072583</td>\n      <td>0.208516</td>\n      <td>0.208516</td>\n      <td>0.025823</td>\n      <td>0.004665</td>\n      <td>0.019050</td>\n      <td>0.032917</td>\n      <td>0.019050</td>\n      <td>1286.461548</td>\n      <td>772.374573</td>\n      <td>0</td>\n      <td>2402</td>\n      <td>1284</td>\n      <td>0.048069</td>\n      <td>0.007596</td>\n      <td>0.036112</td>\n      <td>0.060770</td>\n      <td>0.036112</td>\n      <td>0.705671</td>\n      <td>0.018540</td>\n      <td>0.684371</td>\n      <td>0.748383</td>\n      <td>0.748383</td>\n      <td>0.046753</td>\n      <td>0.024456</td>\n      <td>0.008499</td>\n      <td>0.073904</td>\n      <td>0.017684</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.005146</td>\n      <td>0.002801</td>\n      <td>0.000004</td>\n      <td>0.009756</td>\n      <td>0.009756</td>\n      <td>0.033809</td>\n      <td>0.052705</td>\n      <td>0.006782</td>\n      <td>0.127805</td>\n      <td>0.127805</td>\n      <td>0.014848</td>\n      <td>0.014395</td>\n      <td>0.001797</td>\n      <td>0.057174</td>\n      <td>0.018667</td>\n      <td>15.923077</td>\n      <td>0.277350</td>\n      <td>15</td>\n      <td>16</td>\n      <td>15</td>\n      <td>0.326530</td>\n      <td>0.221335</td>\n      <td>0.059118</td>\n      <td>0.857541</td>\n      <td>0.411989</td>\n      <td>0.053319</td>\n      <td>0.030845</td>\n      <td>0.015966</td>\n      <td>0.103947</td>\n      <td>0.048978</td>\n      <td>0.005560</td>\n      <td>0.002920</td>\n      <td>0.000095</td>\n      <td>0.009642</td>\n      <td>0.009538</td>\n      <td>14.230769</td>\n      <td>3.244324</td>\n      <td>10</td>\n      <td>23</td>\n      <td>...</td>\n      <td>-0.929122</td>\n      <td>-0.861109</td>\n      <td>-0.880519</td>\n      <td>-0.566665</td>\n      <td>0.036880</td>\n      <td>-0.619012</td>\n      <td>-0.510142</td>\n      <td>-0.570898</td>\n      <td>-0.895608</td>\n      <td>0.022676</td>\n      <td>-0.927894</td>\n      <td>-0.852205</td>\n      <td>-0.873588</td>\n      <td>-0.562453</td>\n      <td>0.036707</td>\n      <td>-0.617784</td>\n      <td>-0.507154</td>\n      <td>-0.563967</td>\n      <td>-0.764205</td>\n      <td>0.022943</td>\n      <td>-0.793682</td>\n      <td>-0.723884</td>\n      <td>-0.747654</td>\n      <td>-0.431051</td>\n      <td>0.038022</td>\n      <td>-0.483571</td>\n      <td>-0.376399</td>\n      <td>-0.438033</td>\n      <td>13</td>\n      <td>0</td>\n      <td>1</td>\n      <td>13</td>\n      <td>2</td>\n      <td>1</td>\n      <td>13</td>\n      <td>1</td>\n      <td>1</td>\n      <td>13</td>\n      <td>0</td>\n      <td>1</td>\n      <td>13</td>\n      <td>0</td>\n      <td>1</td>\n      <td>13</td>\n      <td>0</td>\n      <td>2</td>\n      <td>13</td>\n      <td>2</td>\n      <td>1</td>\n      <td>13</td>\n      <td>3</td>\n      <td>1</td>\n      <td>13</td>\n      <td>0</td>\n      <td>1</td>\n      <td>13</td>\n      <td>-1</td>\n      <td>1</td>\n      <td>13</td>\n      <td>6</td>\n      <td>1</td>\n      <td>0.001663</td>\n      <td>-12.0</td>\n      <td>0.000126</td>\n      <td>-0.001391</td>\n      <td>-0.000337</td>\n      <td>0.006024</td>\n      <td>0.0</td>\n      <td>-0.003665</td>\n      <td>NaN</td>\n      <td>-0.001382</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.002539</td>\n      <td>0.016845</td>\n      <td>0.0</td>\n      <td>-0.031395</td>\n      <td>0.003588</td>\n      <td>0.004579</td>\n      <td>0.0</td>\n      <td>-0.044039</td>\n      <td>-0.004767</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.001192</td>\n      <td>0.0</td>\n      <td>0.003206</td>\n      <td>0.053055</td>\n      <td>-0.004486</td>\n      <td>NaN</td>\n      <td>0.037568</td>\n      <td>-0.000448</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.072336</td>\n      <td>-0.003547</td>\n      <td>288.0</td>\n      <td>-0.011404</td>\n      <td>0.052660</td>\n      <td>0.009185</td>\n      <td>0.0</td>\n      <td>0.001132</td>\n      <td>NaN</td>\n      <td>-0.004625</td>\n      <td>-1.0</td>\n      <td>-0.112783</td>\n      <td>-0.054044</td>\n      <td>0.005553</td>\n      <td>-2.0</td>\n      <td>-0.004542</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.003950</td>\n      <td>0.005626</td>\n      <td>0.0</td>\n      <td>-0.002191</td>\n      <td>0.001692</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.002879</td>\n      <td>0.0</td>\n      <td>-1.0</td>\n      <td>-0.000241</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>-0.006784</td>\n      <td>0.0</td>\n      <td>0.003503</td>\n      <td>-0.011997</td>\n      <td>-0.007419</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.006864</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.005359</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.001794</td>\n      <td>0.0</td>\n      <td>0.004686</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.004191</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.073286</td>\n      <td>-0.007379</td>\n      <td>-0.080668</td>\n      <td>0.001961</td>\n      <td>0.001206</td>\n      <td>0.000189</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.002813</td>\n      <td>0.002615</td>\n      <td>0.0</td>\n      <td>-0.001804</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.000006</td>\n      <td>0.288733</td>\n      <td>0.0</td>\n      <td>0.005435</td>\n      <td>0.007249</td>\n      <td>-0.007188</td>\n      <td>0.011845</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.007516</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>-0.006597</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.000641</td>\n      <td>0.0</td>\n      <td>-10.536292</td>\n      <td>0.0</td>\n      <td>33523.0</td>\n      <td>-0.002111</td>\n      <td>-0.053503</td>\n      <td>-0.006288</td>\n      <td>-0.057681</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-12.001662</td>\n      <td>-12.053055</td>\n      <td>-0.001663</td>\n      <td>-0.053055</td>\n      <td>0.005201</td>\n      <td>-0.046191</td>\n      <td>-0.009042</td>\n      <td>-0.060434</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00001b22f846c82c51f6e3958ccd81970162bae8b007e8...</td>\n      <td>0.878454</td>\n      <td>0.028911</td>\n      <td>0.797670</td>\n      <td>0.904482</td>\n      <td>0.880875</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.004386</td>\n      <td>0.002786</td>\n      <td>0.001472</td>\n      <td>0.009997</td>\n      <td>0.004284</td>\n      <td>0.815677</td>\n      <td>0.003545</td>\n      <td>0.810796</td>\n      <td>0.819987</td>\n      <td>0.812649</td>\n      <td>0.006621</td>\n      <td>0.001919</td>\n      <td>0.003540</td>\n      <td>0.009443</td>\n      <td>0.006450</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.005493</td>\n      <td>0.002834</td>\n      <td>0.000626</td>\n      <td>0.009383</td>\n      <td>0.007196</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.076923</td>\n      <td>0.27735</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2.230769</td>\n      <td>1.690850</td>\n      <td>1</td>\n      <td>7</td>\n      <td>2</td>\n      <td>0.236871</td>\n      <td>0.008896</td>\n      <td>0.222406</td>\n      <td>0.251598</td>\n      <td>0.251598</td>\n      <td>0.004618</td>\n      <td>0.003043</td>\n      <td>0.000215</td>\n      <td>0.008656</td>\n      <td>0.001557</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.464475</td>\n      <td>0.060166</td>\n      <td>0.413028</td>\n      <td>0.647064</td>\n      <td>0.433713</td>\n      <td>0.328617</td>\n      <td>0.007183</td>\n      <td>0.318290</td>\n      <td>0.339566</td>\n      <td>0.339125</td>\n      <td>0.092284</td>\n      <td>0.060616</td>\n      <td>0.030227</td>\n      <td>0.255134</td>\n      <td>0.080370</td>\n      <td>-1.0</td>\n      <td>0.0</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>0.176674</td>\n      <td>0.024615</td>\n      <td>0.129857</td>\n      <td>0.213943</td>\n      <td>0.183628</td>\n      <td>0.034433</td>\n      <td>0.015459</td>\n      <td>0.021261</td>\n      <td>0.079764</td>\n      <td>0.026981</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.615385</td>\n      <td>0.506370</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.004730</td>\n      <td>0.003302</td>\n      <td>0.000422</td>\n      <td>0.009521</td>\n      <td>0.009392</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.199863</td>\n      <td>0.002990</td>\n      <td>0.195188</td>\n      <td>0.203649</td>\n      <td>0.202159</td>\n      <td>0.618191</td>\n      <td>0.075604</td>\n      <td>0.381123</td>\n      <td>0.678706</td>\n      <td>0.628938</td>\n      <td>0.273711</td>\n      <td>0.052875</td>\n      <td>0.162125</td>\n      <td>0.302619</td>\n      <td>0.296313</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.005948</td>\n      <td>0.002943</td>\n      <td>0.001054</td>\n      <td>0.008730</td>\n      <td>0.001824</td>\n      <td>0.004683</td>\n      <td>0.002312</td>\n      <td>0.000111</td>\n      <td>0.007619</td>\n      <td>0.005092</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.011541</td>\n      <td>0.002969</td>\n      <td>0.006100</td>\n      <td>0.015486</td>\n      <td>0.007158</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.077362</td>\n      <td>0.016318</td>\n      <td>0.057529</td>\n      <td>0.099230</td>\n      <td>0.098963</td>\n      <td>0.208154</td>\n      <td>0.003188</td>\n      <td>0.201530</td>\n      <td>0.211538</td>\n      <td>0.209386</td>\n      <td>0.003778</td>\n      <td>0.002688</td>\n      <td>0.000427</td>\n      <td>0.008332</td>\n      <td>0.001749</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.023569</td>\n      <td>0.037544</td>\n      <td>0.000726</td>\n      <td>0.093983</td>\n      <td>0.002847</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.004729</td>\n      <td>0.003074</td>\n      <td>0.000684</td>\n      <td>0.008507</td>\n      <td>0.006699</td>\n      <td>15.923077</td>\n      <td>0.277350</td>\n      <td>15</td>\n      <td>16</td>\n      <td>15</td>\n      <td>0.004735</td>\n      <td>0.002602</td>\n      <td>0.000553</td>\n      <td>0.008550</td>\n      <td>0.002820</td>\n      <td>0.109526</td>\n      <td>0.061762</td>\n      <td>0.040357</td>\n      <td>0.249231</td>\n      <td>0.137834</td>\n      <td>0.004716</td>\n      <td>0.002986</td>\n      <td>0.000019</td>\n      <td>0.009969</td>\n      <td>0.006031</td>\n      <td>12.000000</td>\n      <td>0.000000</td>\n      <td>12</td>\n      <td>12</td>\n      <td>...</td>\n      <td>-0.904482</td>\n      <td>-0.797670</td>\n      <td>-0.880875</td>\n      <td>-0.618191</td>\n      <td>0.075604</td>\n      <td>-0.678706</td>\n      <td>-0.381123</td>\n      <td>-0.628938</td>\n      <td>-0.874368</td>\n      <td>0.029500</td>\n      <td>-0.903706</td>\n      <td>-0.793167</td>\n      <td>-0.871569</td>\n      <td>-0.614105</td>\n      <td>0.075772</td>\n      <td>-0.671846</td>\n      <td>-0.376619</td>\n      <td>-0.619633</td>\n      <td>-0.743706</td>\n      <td>0.030058</td>\n      <td>-0.772993</td>\n      <td>-0.657581</td>\n      <td>-0.748183</td>\n      <td>-0.483443</td>\n      <td>0.077091</td>\n      <td>-0.544554</td>\n      <td>-0.241034</td>\n      <td>-0.496247</td>\n      <td>13</td>\n      <td>0</td>\n      <td>1</td>\n      <td>13</td>\n      <td>1</td>\n      <td>1</td>\n      <td>13</td>\n      <td>1</td>\n      <td>2</td>\n      <td>13</td>\n      <td>0</td>\n      <td>1</td>\n      <td>13</td>\n      <td>0</td>\n      <td>1</td>\n      <td>13</td>\n      <td>0</td>\n      <td>1</td>\n      <td>13</td>\n      <td>2</td>\n      <td>1</td>\n      <td>13</td>\n      <td>3</td>\n      <td>1</td>\n      <td>13</td>\n      <td>2</td>\n      <td>1</td>\n      <td>13</td>\n      <td>-1</td>\n      <td>1</td>\n      <td>13</td>\n      <td>6</td>\n      <td>1</td>\n      <td>0.014532</td>\n      <td>0.0</td>\n      <td>-0.004034</td>\n      <td>-0.006303</td>\n      <td>-0.000306</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.002823</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>-2.0</td>\n      <td>0.006082</td>\n      <td>0.001342</td>\n      <td>0.0</td>\n      <td>-0.053204</td>\n      <td>-0.000441</td>\n      <td>-0.059945</td>\n      <td>0.0</td>\n      <td>0.005560</td>\n      <td>-0.015999</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.008970</td>\n      <td>0.0</td>\n      <td>0.006024</td>\n      <td>0.041822</td>\n      <td>-0.005711</td>\n      <td>NaN</td>\n      <td>-0.002897</td>\n      <td>-0.002436</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0.001058</td>\n      <td>0.0</td>\n      <td>-0.000267</td>\n      <td>-0.002152</td>\n      <td>-0.000395</td>\n      <td>0.0</td>\n      <td>-0.006358</td>\n      <td>NaN</td>\n      <td>0.004024</td>\n      <td>-1.0</td>\n      <td>-0.005039</td>\n      <td>0.060685</td>\n      <td>0.002519</td>\n      <td>0.0</td>\n      <td>-0.002247</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.003069</td>\n      <td>0.007775</td>\n      <td>0.0</td>\n      <td>-0.006896</td>\n      <td>0.008785</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.001042</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.007777</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>-0.002833</td>\n      <td>0.0</td>\n      <td>0.005400</td>\n      <td>0.006263</td>\n      <td>0.006181</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.007835</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.000455</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.006686</td>\n      <td>0.0</td>\n      <td>-0.020807</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.002523</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000909</td>\n      <td>-0.001670</td>\n      <td>0.000263</td>\n      <td>-0.001102</td>\n      <td>-0.002282</td>\n      <td>0.005371</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.000963</td>\n      <td>-0.005231</td>\n      <td>0.0</td>\n      <td>-0.001479</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.014236</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>-0.000573</td>\n      <td>0.007793</td>\n      <td>0.009068</td>\n      <td>0.001451</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0.007902</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>-0.006491</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.016968</td>\n      <td>-0.044258</td>\n      <td>-0.010508</td>\n      <td>-0.037797</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-0.014532</td>\n      <td>-0.041822</td>\n      <td>-0.014532</td>\n      <td>-0.041822</td>\n      <td>-0.006697</td>\n      <td>-0.033986</td>\n      <td>-0.016202</td>\n      <td>-0.043491</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>000041bdba6ecadd89a52d11886e8eaaec9325906c9723...</td>\n      <td>0.598969</td>\n      <td>0.020107</td>\n      <td>0.567442</td>\n      <td>0.623392</td>\n      <td>0.621776</td>\n      <td>1.538462</td>\n      <td>3.017045</td>\n      <td>0</td>\n      <td>9</td>\n      <td>0</td>\n      <td>0.059876</td>\n      <td>0.080531</td>\n      <td>0.005910</td>\n      <td>0.279991</td>\n      <td>0.012564</td>\n      <td>0.955264</td>\n      <td>0.080981</td>\n      <td>0.812053</td>\n      <td>1.009999</td>\n      <td>1.006183</td>\n      <td>0.005665</td>\n      <td>0.003473</td>\n      <td>0.000199</td>\n      <td>0.009915</td>\n      <td>0.007829</td>\n      <td>0.247750</td>\n      <td>0.095122</td>\n      <td>0.149216</td>\n      <td>0.407420</td>\n      <td>0.287766</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.006423</td>\n      <td>0.003360</td>\n      <td>0.000053</td>\n      <td>0.010927</td>\n      <td>0.009937</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.061026</td>\n      <td>0.041993</td>\n      <td>0.006633</td>\n      <td>0.149891</td>\n      <td>0.046104</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2.230769</td>\n      <td>2.832956</td>\n      <td>0</td>\n      <td>8</td>\n      <td>0</td>\n      <td>0.069334</td>\n      <td>0.008501</td>\n      <td>0.056394</td>\n      <td>0.085103</td>\n      <td>0.085103</td>\n      <td>0.088374</td>\n      <td>0.074462</td>\n      <td>0.000228</td>\n      <td>0.283781</td>\n      <td>0.118818</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.431905</td>\n      <td>0.030525</td>\n      <td>0.384254</td>\n      <td>0.471676</td>\n      <td>0.410723</td>\n      <td>0.403269</td>\n      <td>0.006355</td>\n      <td>0.392230</td>\n      <td>0.414224</td>\n      <td>0.414224</td>\n      <td>0.076686</td>\n      <td>0.063902</td>\n      <td>0.005276</td>\n      <td>0.177252</td>\n      <td>0.013057</td>\n      <td>-1.0</td>\n      <td>0.0</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>0.160625</td>\n      <td>0.031266</td>\n      <td>0.079987</td>\n      <td>0.196887</td>\n      <td>0.174331</td>\n      <td>0.062130</td>\n      <td>0.073590</td>\n      <td>0.004301</td>\n      <td>0.252338</td>\n      <td>0.011969</td>\n      <td>1.004676</td>\n      <td>0.001928</td>\n      <td>1.002021</td>\n      <td>1.008767</td>\n      <td>1.005561</td>\n      <td>0.439581</td>\n      <td>0.044539</td>\n      <td>0.341256</td>\n      <td>0.482535</td>\n      <td>0.430318</td>\n      <td>0.076923</td>\n      <td>0.277350</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.052241</td>\n      <td>0.053342</td>\n      <td>0.001702</td>\n      <td>0.176352</td>\n      <td>0.020526</td>\n      <td>0.615385</td>\n      <td>0.650444</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0.199698</td>\n      <td>0.002130</td>\n      <td>0.195300</td>\n      <td>0.203203</td>\n      <td>0.198356</td>\n      <td>0.610934</td>\n      <td>0.090090</td>\n      <td>0.345100</td>\n      <td>0.704214</td>\n      <td>0.672080</td>\n      <td>0.306553</td>\n      <td>0.079528</td>\n      <td>0.192981</td>\n      <td>0.431901</td>\n      <td>0.411625</td>\n      <td>0.004336</td>\n      <td>0.003589</td>\n      <td>0.000346</td>\n      <td>0.00999</td>\n      <td>0.001379</td>\n      <td>0.056297</td>\n      <td>0.044583</td>\n      <td>0.002999</td>\n      <td>0.150845</td>\n      <td>0.022970</td>\n      <td>0.044294</td>\n      <td>0.071076</td>\n      <td>0.000672</td>\n      <td>0.241378</td>\n      <td>0.005491</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.261497</td>\n      <td>0.078128</td>\n      <td>0.152622</td>\n      <td>0.370595</td>\n      <td>0.279464</td>\n      <td>0.048949</td>\n      <td>0.025280</td>\n      <td>0.009411</td>\n      <td>0.077831</td>\n      <td>0.074835</td>\n      <td>961.307678</td>\n      <td>405.585052</td>\n      <td>528</td>\n      <td>1511</td>\n      <td>528</td>\n      <td>0.061726</td>\n      <td>0.018374</td>\n      <td>0.021400</td>\n      <td>0.094076</td>\n      <td>0.021400</td>\n      <td>0.564632</td>\n      <td>0.018147</td>\n      <td>0.533675</td>\n      <td>0.580167</td>\n      <td>0.554483</td>\n      <td>0.081928</td>\n      <td>0.041875</td>\n      <td>0.013755</td>\n      <td>0.124311</td>\n      <td>0.055897</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.023349</td>\n      <td>0.034747</td>\n      <td>0.000053</td>\n      <td>0.088388</td>\n      <td>0.009294</td>\n      <td>0.016887</td>\n      <td>0.008305</td>\n      <td>0.005059</td>\n      <td>0.031257</td>\n      <td>0.011429</td>\n      <td>0.033350</td>\n      <td>0.029768</td>\n      <td>0.006169</td>\n      <td>0.103393</td>\n      <td>0.017101</td>\n      <td>26.538462</td>\n      <td>2.025479</td>\n      <td>24</td>\n      <td>29</td>\n      <td>29</td>\n      <td>0.673302</td>\n      <td>0.331873</td>\n      <td>0.081805</td>\n      <td>1.008510</td>\n      <td>0.394758</td>\n      <td>0.066872</td>\n      <td>0.050442</td>\n      <td>0.026844</td>\n      <td>0.171638</td>\n      <td>0.026844</td>\n      <td>0.004382</td>\n      <td>0.003003</td>\n      <td>0.000218</td>\n      <td>0.009221</td>\n      <td>0.002199</td>\n      <td>12.461538</td>\n      <td>1.664101</td>\n      <td>10</td>\n      <td>14</td>\n      <td>...</td>\n      <td>-0.623392</td>\n      <td>-0.567442</td>\n      <td>-0.621776</td>\n      <td>-0.610934</td>\n      <td>0.090090</td>\n      <td>-0.704214</td>\n      <td>-0.345100</td>\n      <td>-0.672080</td>\n      <td>-0.593470</td>\n      <td>0.020696</td>\n      <td>-0.622428</td>\n      <td>-0.561958</td>\n      <td>-0.615063</td>\n      <td>-0.605434</td>\n      <td>0.088259</td>\n      <td>-0.694389</td>\n      <td>-0.344773</td>\n      <td>-0.665367</td>\n      <td>-0.461909</td>\n      <td>0.021377</td>\n      <td>-0.488943</td>\n      <td>-0.427884</td>\n      <td>-0.488943</td>\n      <td>-0.473874</td>\n      <td>0.090423</td>\n      <td>-0.569697</td>\n      <td>-0.208708</td>\n      <td>-0.539247</td>\n      <td>13</td>\n      <td>0</td>\n      <td>1</td>\n      <td>13</td>\n      <td>2</td>\n      <td>1</td>\n      <td>13</td>\n      <td>1</td>\n      <td>1</td>\n      <td>13</td>\n      <td>0</td>\n      <td>1</td>\n      <td>13</td>\n      <td>7</td>\n      <td>2</td>\n      <td>13</td>\n      <td>0</td>\n      <td>1</td>\n      <td>13</td>\n      <td>2</td>\n      <td>1</td>\n      <td>13</td>\n      <td>3</td>\n      <td>1</td>\n      <td>13</td>\n      <td>0</td>\n      <td>1</td>\n      <td>13</td>\n      <td>-1</td>\n      <td>1</td>\n      <td>13</td>\n      <td>3</td>\n      <td>3</td>\n      <td>-0.001615</td>\n      <td>0.0</td>\n      <td>-0.002025</td>\n      <td>-0.003816</td>\n      <td>0.004060</td>\n      <td>-0.027694</td>\n      <td>0.0</td>\n      <td>0.005133</td>\n      <td>NaN</td>\n      <td>-0.008052</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.010168</td>\n      <td>0.002863</td>\n      <td>0.0</td>\n      <td>-0.015666</td>\n      <td>0.004188</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>-0.021213</td>\n      <td>-0.000396</td>\n      <td>0.000019</td>\n      <td>-0.016143</td>\n      <td>1.0</td>\n      <td>-0.013831</td>\n      <td>2.0</td>\n      <td>0.000415</td>\n      <td>-0.010977</td>\n      <td>0.009746</td>\n      <td>-0.005053</td>\n      <td>-0.007352</td>\n      <td>0.000565</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.026107</td>\n      <td>-0.000784</td>\n      <td>0.0</td>\n      <td>-0.021855</td>\n      <td>0.017433</td>\n      <td>-0.004979</td>\n      <td>0.0</td>\n      <td>0.002537</td>\n      <td>0.00637</td>\n      <td>0.001067</td>\n      <td>0.0</td>\n      <td>-0.490735</td>\n      <td>-0.062369</td>\n      <td>-0.000796</td>\n      <td>0.0</td>\n      <td>-0.000617</td>\n      <td>0.0</td>\n      <td>-1.0</td>\n      <td>0.318869</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.295977</td>\n      <td>-0.003092</td>\n      <td>0.0</td>\n      <td>-0.004697</td>\n      <td>-0.000140</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.001836</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.001609</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0.002884</td>\n      <td>0.0</td>\n      <td>-0.003236</td>\n      <td>-0.022100</td>\n      <td>0.001808</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.003583</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.007059</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.003391</td>\n      <td>0.0</td>\n      <td>-0.006715</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.004253</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.007152</td>\n      <td>-0.006135</td>\n      <td>0.007993</td>\n      <td>-0.003940</td>\n      <td>0.040299</td>\n      <td>-0.001329</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.003921</td>\n      <td>-0.005150</td>\n      <td>0.0</td>\n      <td>-0.003183</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.025346</td>\n      <td>0.058123</td>\n      <td>0.0</td>\n      <td>0.006993</td>\n      <td>-0.001055</td>\n      <td>0.005564</td>\n      <td>0.000747</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.001377</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0.000632</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.000741</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.002180</td>\n      <td>0.011542</td>\n      <td>0.002683</td>\n      <td>0.012044</td>\n      <td>0.320485</td>\n      <td>0.329846</td>\n      <td>0.001615</td>\n      <td>0.010977</td>\n      <td>0.001615</td>\n      <td>0.010977</td>\n      <td>0.005198</td>\n      <td>0.014560</td>\n      <td>-0.004520</td>\n      <td>0.004842</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8a...</td>\n      <td>0.891679</td>\n      <td>0.042325</td>\n      <td>0.805045</td>\n      <td>0.940382</td>\n      <td>0.871900</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.005941</td>\n      <td>0.002475</td>\n      <td>0.000776</td>\n      <td>0.009806</td>\n      <td>0.007679</td>\n      <td>0.814543</td>\n      <td>0.003143</td>\n      <td>0.810670</td>\n      <td>0.819947</td>\n      <td>0.815746</td>\n      <td>0.004180</td>\n      <td>0.002581</td>\n      <td>0.000336</td>\n      <td>0.009076</td>\n      <td>0.001247</td>\n      <td>0.173102</td>\n      <td>0.004669</td>\n      <td>0.166190</td>\n      <td>0.176403</td>\n      <td>0.176403</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.005088</td>\n      <td>0.002910</td>\n      <td>0.000049</td>\n      <td>0.009686</td>\n      <td>0.005528</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.048778</td>\n      <td>0.006847</td>\n      <td>0.037001</td>\n      <td>0.061963</td>\n      <td>0.044671</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>11.692307</td>\n      <td>9.384248</td>\n      <td>3</td>\n      <td>25</td>\n      <td>21</td>\n      <td>0.209150</td>\n      <td>0.117203</td>\n      <td>0.063150</td>\n      <td>0.305305</td>\n      <td>0.069952</td>\n      <td>0.004572</td>\n      <td>0.002297</td>\n      <td>0.001201</td>\n      <td>0.007830</td>\n      <td>0.004855</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.474523</td>\n      <td>0.076167</td>\n      <td>0.366783</td>\n      <td>0.694332</td>\n      <td>0.465525</td>\n      <td>0.471961</td>\n      <td>0.007588</td>\n      <td>0.461473</td>\n      <td>0.484715</td>\n      <td>0.480303</td>\n      <td>0.253697</td>\n      <td>0.093176</td>\n      <td>0.137840</td>\n      <td>0.491528</td>\n      <td>0.325121</td>\n      <td>-1.0</td>\n      <td>0.0</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>0.075672</td>\n      <td>0.046857</td>\n      <td>0.030852</td>\n      <td>0.195757</td>\n      <td>0.048857</td>\n      <td>0.115290</td>\n      <td>0.070823</td>\n      <td>0.035662</td>\n      <td>0.216773</td>\n      <td>0.159818</td>\n      <td>0.386868</td>\n      <td>0.509339</td>\n      <td>0.000000</td>\n      <td>1.008826</td>\n      <td>1.005185</td>\n      <td>0.093218</td>\n      <td>0.020103</td>\n      <td>0.073834</td>\n      <td>0.136212</td>\n      <td>0.095238</td>\n      <td>0.153846</td>\n      <td>0.375534</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.006685</td>\n      <td>0.002242</td>\n      <td>0.002925</td>\n      <td>0.009847</td>\n      <td>0.004027</td>\n      <td>0.153846</td>\n      <td>0.375534</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.233470</td>\n      <td>0.028414</td>\n      <td>0.191802</td>\n      <td>0.256440</td>\n      <td>0.253811</td>\n      <td>0.527254</td>\n      <td>0.088509</td>\n      <td>0.254276</td>\n      <td>0.584359</td>\n      <td>0.570419</td>\n      <td>0.100315</td>\n      <td>0.074579</td>\n      <td>0.044728</td>\n      <td>0.260673</td>\n      <td>0.125195</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.005051</td>\n      <td>0.002665</td>\n      <td>0.002389</td>\n      <td>0.009350</td>\n      <td>0.009350</td>\n      <td>0.005017</td>\n      <td>0.003694</td>\n      <td>0.000714</td>\n      <td>0.009807</td>\n      <td>0.001001</td>\n      <td>0.846154</td>\n      <td>0.375534</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.120290</td>\n      <td>0.008589</td>\n      <td>0.108082</td>\n      <td>0.128201</td>\n      <td>0.122915</td>\n      <td>0.049640</td>\n      <td>0.060154</td>\n      <td>0.005756</td>\n      <td>0.151135</td>\n      <td>0.013041</td>\n      <td>157.076920</td>\n      <td>383.420013</td>\n      <td>0</td>\n      <td>1021</td>\n      <td>0</td>\n      <td>0.203298</td>\n      <td>0.041725</td>\n      <td>0.125503</td>\n      <td>0.254067</td>\n      <td>0.254067</td>\n      <td>0.178482</td>\n      <td>0.009615</td>\n      <td>0.163719</td>\n      <td>0.190924</td>\n      <td>0.183075</td>\n      <td>0.004422</td>\n      <td>0.002974</td>\n      <td>0.000626</td>\n      <td>0.008859</td>\n      <td>0.006051</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.318151</td>\n      <td>0.102317</td>\n      <td>0.094102</td>\n      <td>0.392473</td>\n      <td>0.382744</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.004924</td>\n      <td>0.003445</td>\n      <td>0.000025</td>\n      <td>0.009628</td>\n      <td>0.009469</td>\n      <td>23.153847</td>\n      <td>3.715870</td>\n      <td>18</td>\n      <td>28</td>\n      <td>28</td>\n      <td>0.003476</td>\n      <td>0.002267</td>\n      <td>0.000846</td>\n      <td>0.009551</td>\n      <td>0.002670</td>\n      <td>0.356445</td>\n      <td>0.255848</td>\n      <td>0.082395</td>\n      <td>0.715081</td>\n      <td>0.600739</td>\n      <td>0.006005</td>\n      <td>0.002529</td>\n      <td>0.001513</td>\n      <td>0.009890</td>\n      <td>0.005842</td>\n      <td>12.538462</td>\n      <td>1.391365</td>\n      <td>12</td>\n      <td>17</td>\n      <td>...</td>\n      <td>-0.940382</td>\n      <td>-0.805045</td>\n      <td>-0.871900</td>\n      <td>-0.527254</td>\n      <td>0.088509</td>\n      <td>-0.584359</td>\n      <td>-0.254276</td>\n      <td>-0.570419</td>\n      <td>-0.887315</td>\n      <td>0.043650</td>\n      <td>-0.937680</td>\n      <td>-0.795330</td>\n      <td>-0.864329</td>\n      <td>-0.522890</td>\n      <td>0.089991</td>\n      <td>-0.581657</td>\n      <td>-0.244561</td>\n      <td>-0.562848</td>\n      <td>-0.756159</td>\n      <td>0.042603</td>\n      <td>-0.805170</td>\n      <td>-0.665572</td>\n      <td>-0.739801</td>\n      <td>-0.391734</td>\n      <td>0.089403</td>\n      <td>-0.444325</td>\n      <td>-0.114802</td>\n      <td>-0.438320</td>\n      <td>13</td>\n      <td>0</td>\n      <td>1</td>\n      <td>13</td>\n      <td>1</td>\n      <td>2</td>\n      <td>13</td>\n      <td>1</td>\n      <td>1</td>\n      <td>13</td>\n      <td>0</td>\n      <td>1</td>\n      <td>13</td>\n      <td>5</td>\n      <td>1</td>\n      <td>13</td>\n      <td>0</td>\n      <td>1</td>\n      <td>13</td>\n      <td>2</td>\n      <td>1</td>\n      <td>13</td>\n      <td>3</td>\n      <td>1</td>\n      <td>13</td>\n      <td>0</td>\n      <td>1</td>\n      <td>13</td>\n      <td>1</td>\n      <td>1</td>\n      <td>13</td>\n      <td>6</td>\n      <td>1</td>\n      <td>-0.007338</td>\n      <td>0.0</td>\n      <td>-0.000843</td>\n      <td>0.002465</td>\n      <td>-0.004237</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.001567</td>\n      <td>NaN</td>\n      <td>0.007670</td>\n      <td>0.0</td>\n      <td>-1.0</td>\n      <td>-0.001607</td>\n      <td>-0.000876</td>\n      <td>0.0</td>\n      <td>0.009716</td>\n      <td>-0.004412</td>\n      <td>0.081822</td>\n      <td>0.0</td>\n      <td>-0.005520</td>\n      <td>-0.004206</td>\n      <td>0.004426</td>\n      <td>-0.001273</td>\n      <td>0.0</td>\n      <td>-0.005526</td>\n      <td>0.0</td>\n      <td>-0.002511</td>\n      <td>0.000418</td>\n      <td>0.000639</td>\n      <td>NaN</td>\n      <td>0.000884</td>\n      <td>-0.008344</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0.003644</td>\n      <td>0.0</td>\n      <td>0.022100</td>\n      <td>-0.005202</td>\n      <td>-0.001676</td>\n      <td>0.0</td>\n      <td>0.002218</td>\n      <td>NaN</td>\n      <td>0.002199</td>\n      <td>1.0</td>\n      <td>-0.001635</td>\n      <td>-0.026399</td>\n      <td>0.001009</td>\n      <td>1.0</td>\n      <td>0.005869</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0.001988</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.006730</td>\n      <td>0.002950</td>\n      <td>0.0</td>\n      <td>0.002785</td>\n      <td>0.004478</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.002538</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.008005</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0.006578</td>\n      <td>0.0</td>\n      <td>-0.002954</td>\n      <td>0.006692</td>\n      <td>0.003931</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.004710</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.003384</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.007219</td>\n      <td>0.0</td>\n      <td>-0.010899</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.005786</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.000005</td>\n      <td>0.000128</td>\n      <td>-0.000984</td>\n      <td>-0.005093</td>\n      <td>-0.007105</td>\n      <td>0.010343</td>\n      <td>0.0</td>\n      <td>-0.000868</td>\n      <td>0.005986</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.007248</td>\n      <td>0.002984</td>\n      <td>0.0</td>\n      <td>0.001717</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.014405</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.003443</td>\n      <td>0.005265</td>\n      <td>0.004413</td>\n      <td>0.006381</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.001335</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.002185</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0.002409</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.000618</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.001006</td>\n      <td>-0.008761</td>\n      <td>0.009537</td>\n      <td>0.001781</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.007338</td>\n      <td>-0.000418</td>\n      <td>0.007338</td>\n      <td>-0.000418</td>\n      <td>0.012047</td>\n      <td>0.004292</td>\n      <td>0.007465</td>\n      <td>-0.000290</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 1163 columns</p>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "1163"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train.columns)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/1164 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0584e5633c8f419aa6821bf4c1bf2ddb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "--------------------------------------------------\n",
      "Training fold 0 with 1452 features...\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 246946\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 1444\n",
      "[LightGBM] [Info] Using requested OpenCL platform 1 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3090, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 847 dense feature groups (296.90 MB) transferred to GPU in 0.133886 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "[500]\ttraining's binary_logloss: 0.334721\ttraining's amex_metric: 0.779609\tvalid_1's binary_logloss: 0.336641\tvalid_1's amex_metric: 0.771418\n",
      "[1000]\ttraining's binary_logloss: 0.244808\ttraining's amex_metric: 0.795365\tvalid_1's binary_logloss: 0.250345\tvalid_1's amex_metric: 0.782031\n",
      "[1500]\ttraining's binary_logloss: 0.221193\ttraining's amex_metric: 0.80814\tvalid_1's binary_logloss: 0.230594\tvalid_1's amex_metric: 0.78856\n",
      "[2000]\ttraining's binary_logloss: 0.207803\ttraining's amex_metric: 0.820496\tvalid_1's binary_logloss: 0.222122\tvalid_1's amex_metric: 0.793236\n",
      "[2500]\ttraining's binary_logloss: 0.200894\ttraining's amex_metric: 0.830982\tvalid_1's binary_logloss: 0.219308\tvalid_1's amex_metric: 0.795063\n",
      "[3000]\ttraining's binary_logloss: 0.193958\ttraining's amex_metric: 0.840868\tvalid_1's binary_logloss: 0.217095\tvalid_1's amex_metric: 0.797646\n",
      "[3500]\ttraining's binary_logloss: 0.187604\ttraining's amex_metric: 0.85076\tvalid_1's binary_logloss: 0.215635\tvalid_1's amex_metric: 0.79817\n",
      "[4000]\ttraining's binary_logloss: 0.181962\ttraining's amex_metric: 0.860999\tvalid_1's binary_logloss: 0.214764\tvalid_1's amex_metric: 0.800267\n",
      "[4500]\ttraining's binary_logloss: 0.176459\ttraining's amex_metric: 0.870361\tvalid_1's binary_logloss: 0.214047\tvalid_1's amex_metric: 0.800737\n",
      "[5000]\ttraining's binary_logloss: 0.170999\ttraining's amex_metric: 0.879839\tvalid_1's binary_logloss: 0.21349\tvalid_1's amex_metric: 0.80048\n",
      "[5500]\ttraining's binary_logloss: 0.1661\ttraining's amex_metric: 0.887886\tvalid_1's binary_logloss: 0.213088\tvalid_1's amex_metric: 0.801217\n",
      "[6000]\ttraining's binary_logloss: 0.161931\ttraining's amex_metric: 0.895649\tvalid_1's binary_logloss: 0.212806\tvalid_1's amex_metric: 0.801052\n",
      "[6500]\ttraining's binary_logloss: 0.157496\ttraining's amex_metric: 0.903062\tvalid_1's binary_logloss: 0.212533\tvalid_1's amex_metric: 0.801767\n",
      "[7000]\ttraining's binary_logloss: 0.152397\ttraining's amex_metric: 0.911471\tvalid_1's binary_logloss: 0.2123\tvalid_1's amex_metric: 0.801456\n",
      "[7500]\ttraining's binary_logloss: 0.14759\ttraining's amex_metric: 0.919477\tvalid_1's binary_logloss: 0.212144\tvalid_1's amex_metric: 0.802426\n",
      "[8000]\ttraining's binary_logloss: 0.143325\ttraining's amex_metric: 0.925977\tvalid_1's binary_logloss: 0.212021\tvalid_1's amex_metric: 0.802414\n",
      "[8500]\ttraining's binary_logloss: 0.139657\ttraining's amex_metric: 0.93245\tvalid_1's binary_logloss: 0.211988\tvalid_1's amex_metric: 0.80162\n",
      "[9000]\ttraining's binary_logloss: 0.135489\ttraining's amex_metric: 0.938626\tvalid_1's binary_logloss: 0.211915\tvalid_1's amex_metric: 0.801535\n",
      "[9500]\ttraining's binary_logloss: 0.131764\ttraining's amex_metric: 0.944107\tvalid_1's binary_logloss: 0.211802\tvalid_1's amex_metric: 0.802121\n",
      "[10000]\ttraining's binary_logloss: 0.128117\ttraining's amex_metric: 0.949449\tvalid_1's binary_logloss: 0.211732\tvalid_1's amex_metric: 0.802565\n",
      "[10500]\ttraining's binary_logloss: 0.124929\ttraining's amex_metric: 0.953758\tvalid_1's binary_logloss: 0.211704\tvalid_1's amex_metric: 0.802792\n",
      "[11000]\ttraining's binary_logloss: 0.121694\ttraining's amex_metric: 0.958478\tvalid_1's binary_logloss: 0.211742\tvalid_1's amex_metric: 0.802609\n",
      "[11500]\ttraining's binary_logloss: 0.118317\ttraining's amex_metric: 0.962903\tvalid_1's binary_logloss: 0.211711\tvalid_1's amex_metric: 0.803157\n",
      "[12000]\ttraining's binary_logloss: 0.11526\ttraining's amex_metric: 0.967098\tvalid_1's binary_logloss: 0.211706\tvalid_1's amex_metric: 0.802848\n",
      "[12500]\ttraining's binary_logloss: 0.112577\ttraining's amex_metric: 0.970496\tvalid_1's binary_logloss: 0.211786\tvalid_1's amex_metric: 0.802262\n",
      "[13000]\ttraining's binary_logloss: 0.10918\ttraining's amex_metric: 0.974054\tvalid_1's binary_logloss: 0.211834\tvalid_1's amex_metric: 0.802021\n",
      "[13500]\ttraining's binary_logloss: 0.106426\ttraining's amex_metric: 0.977333\tvalid_1's binary_logloss: 0.211856\tvalid_1's amex_metric: 0.802316\n",
      "[14000]\ttraining's binary_logloss: 0.103423\ttraining's amex_metric: 0.980424\tvalid_1's binary_logloss: 0.211949\tvalid_1's amex_metric: 0.802499\n",
      "[14500]\ttraining's binary_logloss: 0.101059\ttraining's amex_metric: 0.982689\tvalid_1's binary_logloss: 0.211961\tvalid_1's amex_metric: 0.802658\n",
      "[15000]\ttraining's binary_logloss: 0.0986609\ttraining's amex_metric: 0.985122\tvalid_1's binary_logloss: 0.212025\tvalid_1's amex_metric: 0.802196\n",
      "Our fold 0 CV score is 0.8021955571858075\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 1 with 1452 features...\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 247058\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 1444\n",
      "[LightGBM] [Info] Using requested OpenCL platform 1 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3090, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 847 dense feature groups (296.90 MB) transferred to GPU in 0.075001 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "[500]\ttraining's binary_logloss: 0.334369\ttraining's amex_metric: 0.781316\tvalid_1's binary_logloss: 0.337693\tvalid_1's amex_metric: 0.764015\n",
      "[1000]\ttraining's binary_logloss: 0.244078\ttraining's amex_metric: 0.79745\tvalid_1's binary_logloss: 0.252079\tvalid_1's amex_metric: 0.775444\n",
      "[1500]\ttraining's binary_logloss: 0.220328\ttraining's amex_metric: 0.81074\tvalid_1's binary_logloss: 0.232959\tvalid_1's amex_metric: 0.780871\n",
      "[2000]\ttraining's binary_logloss: 0.20685\ttraining's amex_metric: 0.823119\tvalid_1's binary_logloss: 0.224873\tvalid_1's amex_metric: 0.784812\n",
      "[2500]\ttraining's binary_logloss: 0.19994\ttraining's amex_metric: 0.832713\tvalid_1's binary_logloss: 0.222269\tvalid_1's amex_metric: 0.786625\n",
      "[3000]\ttraining's binary_logloss: 0.193025\ttraining's amex_metric: 0.842474\tvalid_1's binary_logloss: 0.220185\tvalid_1's amex_metric: 0.789792\n",
      "[3500]\ttraining's binary_logloss: 0.186579\ttraining's amex_metric: 0.853115\tvalid_1's binary_logloss: 0.218888\tvalid_1's amex_metric: 0.790517\n",
      "[4000]\ttraining's binary_logloss: 0.180894\ttraining's amex_metric: 0.862641\tvalid_1's binary_logloss: 0.218064\tvalid_1's amex_metric: 0.791324\n",
      "[4500]\ttraining's binary_logloss: 0.175392\ttraining's amex_metric: 0.871984\tvalid_1's binary_logloss: 0.217535\tvalid_1's amex_metric: 0.791545\n",
      "[5000]\ttraining's binary_logloss: 0.16996\ttraining's amex_metric: 0.88111\tvalid_1's binary_logloss: 0.217059\tvalid_1's amex_metric: 0.792876\n",
      "[5500]\ttraining's binary_logloss: 0.165102\ttraining's amex_metric: 0.88922\tvalid_1's binary_logloss: 0.216735\tvalid_1's amex_metric: 0.793481\n",
      "[6000]\ttraining's binary_logloss: 0.160921\ttraining's amex_metric: 0.897147\tvalid_1's binary_logloss: 0.21655\tvalid_1's amex_metric: 0.793284\n",
      "[6500]\ttraining's binary_logloss: 0.156546\ttraining's amex_metric: 0.904193\tvalid_1's binary_logloss: 0.216346\tvalid_1's amex_metric: 0.794093\n",
      "[7000]\ttraining's binary_logloss: 0.151473\ttraining's amex_metric: 0.912449\tvalid_1's binary_logloss: 0.216104\tvalid_1's amex_metric: 0.794312\n",
      "[7500]\ttraining's binary_logloss: 0.146652\ttraining's amex_metric: 0.920054\tvalid_1's binary_logloss: 0.216008\tvalid_1's amex_metric: 0.79423\n",
      "[8000]\ttraining's binary_logloss: 0.142374\ttraining's amex_metric: 0.927\tvalid_1's binary_logloss: 0.215906\tvalid_1's amex_metric: 0.794727\n",
      "[8500]\ttraining's binary_logloss: 0.138707\ttraining's amex_metric: 0.933382\tvalid_1's binary_logloss: 0.215833\tvalid_1's amex_metric: 0.794794\n",
      "[9000]\ttraining's binary_logloss: 0.13456\ttraining's amex_metric: 0.939189\tvalid_1's binary_logloss: 0.215761\tvalid_1's amex_metric: 0.794777\n",
      "[9500]\ttraining's binary_logloss: 0.130876\ttraining's amex_metric: 0.944995\tvalid_1's binary_logloss: 0.215707\tvalid_1's amex_metric: 0.794841\n",
      "[10000]\ttraining's binary_logloss: 0.127249\ttraining's amex_metric: 0.950354\tvalid_1's binary_logloss: 0.215677\tvalid_1's amex_metric: 0.794922\n",
      "[10500]\ttraining's binary_logloss: 0.124058\ttraining's amex_metric: 0.95471\tvalid_1's binary_logloss: 0.215713\tvalid_1's amex_metric: 0.795465\n",
      "[11000]\ttraining's binary_logloss: 0.1208\ttraining's amex_metric: 0.959203\tvalid_1's binary_logloss: 0.215728\tvalid_1's amex_metric: 0.795069\n",
      "[11500]\ttraining's binary_logloss: 0.117438\ttraining's amex_metric: 0.963786\tvalid_1's binary_logloss: 0.215773\tvalid_1's amex_metric: 0.795467\n",
      "[12000]\ttraining's binary_logloss: 0.114383\ttraining's amex_metric: 0.967597\tvalid_1's binary_logloss: 0.215869\tvalid_1's amex_metric: 0.794731\n",
      "[12500]\ttraining's binary_logloss: 0.111725\ttraining's amex_metric: 0.971045\tvalid_1's binary_logloss: 0.215925\tvalid_1's amex_metric: 0.794361\n",
      "[13000]\ttraining's binary_logloss: 0.1083\ttraining's amex_metric: 0.974764\tvalid_1's binary_logloss: 0.216048\tvalid_1's amex_metric: 0.794668\n",
      "[13500]\ttraining's binary_logloss: 0.105594\ttraining's amex_metric: 0.978087\tvalid_1's binary_logloss: 0.216136\tvalid_1's amex_metric: 0.795008\n",
      "[14000]\ttraining's binary_logloss: 0.102559\ttraining's amex_metric: 0.981015\tvalid_1's binary_logloss: 0.216244\tvalid_1's amex_metric: 0.795034\n",
      "[14500]\ttraining's binary_logloss: 0.100208\ttraining's amex_metric: 0.983387\tvalid_1's binary_logloss: 0.21626\tvalid_1's amex_metric: 0.794337\n",
      "[15000]\ttraining's binary_logloss: 0.0978575\ttraining's amex_metric: 0.985492\tvalid_1's binary_logloss: 0.216322\tvalid_1's amex_metric: 0.794532\n",
      "Our fold 1 CV score is 0.794531537252939\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 2 with 1452 features...\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 247015\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 1444\n",
      "[LightGBM] [Info] Using requested OpenCL platform 1 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3090, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 848 dense feature groups (296.90 MB) transferred to GPU in 0.079236 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "[500]\ttraining's binary_logloss: 0.334383\ttraining's amex_metric: 0.779287\tvalid_1's binary_logloss: 0.337973\tvalid_1's amex_metric: 0.768216\n",
      "[1000]\ttraining's binary_logloss: 0.24409\ttraining's amex_metric: 0.795815\tvalid_1's binary_logloss: 0.252177\tvalid_1's amex_metric: 0.777924\n",
      "[1500]\ttraining's binary_logloss: 0.22045\ttraining's amex_metric: 0.809019\tvalid_1's binary_logloss: 0.232643\tvalid_1's amex_metric: 0.784406\n",
      "[2000]\ttraining's binary_logloss: 0.207103\ttraining's amex_metric: 0.821434\tvalid_1's binary_logloss: 0.224315\tvalid_1's amex_metric: 0.788875\n",
      "[2500]\ttraining's binary_logloss: 0.200205\ttraining's amex_metric: 0.832168\tvalid_1's binary_logloss: 0.221626\tvalid_1's amex_metric: 0.791449\n",
      "[3000]\ttraining's binary_logloss: 0.193313\ttraining's amex_metric: 0.842365\tvalid_1's binary_logloss: 0.219496\tvalid_1's amex_metric: 0.793684\n",
      "[3500]\ttraining's binary_logloss: 0.186952\ttraining's amex_metric: 0.852503\tvalid_1's binary_logloss: 0.218048\tvalid_1's amex_metric: 0.794967\n",
      "[4000]\ttraining's binary_logloss: 0.181257\ttraining's amex_metric: 0.861895\tvalid_1's binary_logloss: 0.217074\tvalid_1's amex_metric: 0.796326\n",
      "[4500]\ttraining's binary_logloss: 0.175718\ttraining's amex_metric: 0.87152\tvalid_1's binary_logloss: 0.21642\tvalid_1's amex_metric: 0.796463\n",
      "[5000]\ttraining's binary_logloss: 0.170261\ttraining's amex_metric: 0.881219\tvalid_1's binary_logloss: 0.215926\tvalid_1's amex_metric: 0.797525\n",
      "[5500]\ttraining's binary_logloss: 0.165424\ttraining's amex_metric: 0.889333\tvalid_1's binary_logloss: 0.215525\tvalid_1's amex_metric: 0.797219\n",
      "[6000]\ttraining's binary_logloss: 0.161232\ttraining's amex_metric: 0.897131\tvalid_1's binary_logloss: 0.215255\tvalid_1's amex_metric: 0.797519\n",
      "[6500]\ttraining's binary_logloss: 0.156835\ttraining's amex_metric: 0.904367\tvalid_1's binary_logloss: 0.215001\tvalid_1's amex_metric: 0.79775\n",
      "[7000]\ttraining's binary_logloss: 0.151783\ttraining's amex_metric: 0.912004\tvalid_1's binary_logloss: 0.214733\tvalid_1's amex_metric: 0.797165\n",
      "[7500]\ttraining's binary_logloss: 0.147\ttraining's amex_metric: 0.920202\tvalid_1's binary_logloss: 0.214587\tvalid_1's amex_metric: 0.796106\n",
      "[8000]\ttraining's binary_logloss: 0.14273\ttraining's amex_metric: 0.927064\tvalid_1's binary_logloss: 0.214424\tvalid_1's amex_metric: 0.796786\n",
      "[8500]\ttraining's binary_logloss: 0.139061\ttraining's amex_metric: 0.933583\tvalid_1's binary_logloss: 0.214322\tvalid_1's amex_metric: 0.796951\n",
      "[9000]\ttraining's binary_logloss: 0.134907\ttraining's amex_metric: 0.939668\tvalid_1's binary_logloss: 0.214221\tvalid_1's amex_metric: 0.796896\n",
      "[9500]\ttraining's binary_logloss: 0.131173\ttraining's amex_metric: 0.945318\tvalid_1's binary_logloss: 0.214215\tvalid_1's amex_metric: 0.796855\n",
      "[10000]\ttraining's binary_logloss: 0.127509\ttraining's amex_metric: 0.950805\tvalid_1's binary_logloss: 0.214209\tvalid_1's amex_metric: 0.796842\n",
      "[10500]\ttraining's binary_logloss: 0.124333\ttraining's amex_metric: 0.955126\tvalid_1's binary_logloss: 0.214177\tvalid_1's amex_metric: 0.797847\n",
      "[11000]\ttraining's binary_logloss: 0.121089\ttraining's amex_metric: 0.959305\tvalid_1's binary_logloss: 0.214209\tvalid_1's amex_metric: 0.797383\n",
      "[11500]\ttraining's binary_logloss: 0.117704\ttraining's amex_metric: 0.963713\tvalid_1's binary_logloss: 0.214233\tvalid_1's amex_metric: 0.797241\n",
      "[12000]\ttraining's binary_logloss: 0.114655\ttraining's amex_metric: 0.967889\tvalid_1's binary_logloss: 0.21426\tvalid_1's amex_metric: 0.797116\n",
      "[12500]\ttraining's binary_logloss: 0.112011\ttraining's amex_metric: 0.971155\tvalid_1's binary_logloss: 0.214252\tvalid_1's amex_metric: 0.797925\n",
      "[13000]\ttraining's binary_logloss: 0.108601\ttraining's amex_metric: 0.974504\tvalid_1's binary_logloss: 0.214359\tvalid_1's amex_metric: 0.797434\n",
      "[13500]\ttraining's binary_logloss: 0.105878\ttraining's amex_metric: 0.977831\tvalid_1's binary_logloss: 0.214424\tvalid_1's amex_metric: 0.797565\n",
      "[14000]\ttraining's binary_logloss: 0.102884\ttraining's amex_metric: 0.980914\tvalid_1's binary_logloss: 0.21452\tvalid_1's amex_metric: 0.796587\n",
      "[14500]\ttraining's binary_logloss: 0.100535\ttraining's amex_metric: 0.983128\tvalid_1's binary_logloss: 0.214603\tvalid_1's amex_metric: 0.797422\n",
      "[15000]\ttraining's binary_logloss: 0.0981666\ttraining's amex_metric: 0.985577\tvalid_1's binary_logloss: 0.21468\tvalid_1's amex_metric: 0.796829\n",
      "Our fold 2 CV score is 0.7968294600542822\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 3 with 1452 features...\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 246959\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 1444\n",
      "[LightGBM] [Info] Using requested OpenCL platform 1 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3090, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 848 dense feature groups (296.90 MB) transferred to GPU in 0.077017 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258935 -> initscore=-1.051512\n",
      "[LightGBM] [Info] Start training from score -1.051512\n",
      "[500]\ttraining's binary_logloss: 0.334049\ttraining's amex_metric: 0.781371\tvalid_1's binary_logloss: 0.338891\tvalid_1's amex_metric: 0.763848\n",
      "[1000]\ttraining's binary_logloss: 0.243851\ttraining's amex_metric: 0.797258\tvalid_1's binary_logloss: 0.253383\tvalid_1's amex_metric: 0.77431\n",
      "[1500]\ttraining's binary_logloss: 0.220147\ttraining's amex_metric: 0.810446\tvalid_1's binary_logloss: 0.234147\tvalid_1's amex_metric: 0.780466\n",
      "[2000]\ttraining's binary_logloss: 0.206795\ttraining's amex_metric: 0.82313\tvalid_1's binary_logloss: 0.225906\tvalid_1's amex_metric: 0.784429\n",
      "[2500]\ttraining's binary_logloss: 0.199871\ttraining's amex_metric: 0.83333\tvalid_1's binary_logloss: 0.223156\tvalid_1's amex_metric: 0.786877\n",
      "[3000]\ttraining's binary_logloss: 0.193\ttraining's amex_metric: 0.84279\tvalid_1's binary_logloss: 0.221043\tvalid_1's amex_metric: 0.788062\n",
      "[3500]\ttraining's binary_logloss: 0.186609\ttraining's amex_metric: 0.853235\tvalid_1's binary_logloss: 0.219597\tvalid_1's amex_metric: 0.789557\n",
      "[4000]\ttraining's binary_logloss: 0.180946\ttraining's amex_metric: 0.86281\tvalid_1's binary_logloss: 0.218686\tvalid_1's amex_metric: 0.790369\n",
      "[4500]\ttraining's binary_logloss: 0.1754\ttraining's amex_metric: 0.872046\tvalid_1's binary_logloss: 0.218038\tvalid_1's amex_metric: 0.790294\n",
      "[5000]\ttraining's binary_logloss: 0.169942\ttraining's amex_metric: 0.880972\tvalid_1's binary_logloss: 0.217474\tvalid_1's amex_metric: 0.791236\n",
      "[5500]\ttraining's binary_logloss: 0.165072\ttraining's amex_metric: 0.889685\tvalid_1's binary_logloss: 0.217135\tvalid_1's amex_metric: 0.791925\n",
      "[6000]\ttraining's binary_logloss: 0.160916\ttraining's amex_metric: 0.897817\tvalid_1's binary_logloss: 0.216901\tvalid_1's amex_metric: 0.791897\n",
      "[6500]\ttraining's binary_logloss: 0.156532\ttraining's amex_metric: 0.904389\tvalid_1's binary_logloss: 0.216675\tvalid_1's amex_metric: 0.792243\n",
      "[7000]\ttraining's binary_logloss: 0.151468\ttraining's amex_metric: 0.912543\tvalid_1's binary_logloss: 0.216436\tvalid_1's amex_metric: 0.792369\n",
      "[7500]\ttraining's binary_logloss: 0.146669\ttraining's amex_metric: 0.920466\tvalid_1's binary_logloss: 0.216278\tvalid_1's amex_metric: 0.792161\n",
      "[8000]\ttraining's binary_logloss: 0.14241\ttraining's amex_metric: 0.927118\tvalid_1's binary_logloss: 0.216144\tvalid_1's amex_metric: 0.791826\n",
      "[8500]\ttraining's binary_logloss: 0.138754\ttraining's amex_metric: 0.933666\tvalid_1's binary_logloss: 0.216019\tvalid_1's amex_metric: 0.79202\n",
      "[9000]\ttraining's binary_logloss: 0.134625\ttraining's amex_metric: 0.939597\tvalid_1's binary_logloss: 0.215937\tvalid_1's amex_metric: 0.792131\n",
      "[9500]\ttraining's binary_logloss: 0.130922\ttraining's amex_metric: 0.945063\tvalid_1's binary_logloss: 0.215939\tvalid_1's amex_metric: 0.791778\n",
      "[10000]\ttraining's binary_logloss: 0.12727\ttraining's amex_metric: 0.950274\tvalid_1's binary_logloss: 0.215881\tvalid_1's amex_metric: 0.791763\n",
      "[10500]\ttraining's binary_logloss: 0.12411\ttraining's amex_metric: 0.954787\tvalid_1's binary_logloss: 0.215923\tvalid_1's amex_metric: 0.792445\n",
      "[11000]\ttraining's binary_logloss: 0.120864\ttraining's amex_metric: 0.959675\tvalid_1's binary_logloss: 0.215946\tvalid_1's amex_metric: 0.79234\n",
      "[11500]\ttraining's binary_logloss: 0.11747\ttraining's amex_metric: 0.964033\tvalid_1's binary_logloss: 0.215963\tvalid_1's amex_metric: 0.792852\n",
      "[12000]\ttraining's binary_logloss: 0.114423\ttraining's amex_metric: 0.967799\tvalid_1's binary_logloss: 0.215955\tvalid_1's amex_metric: 0.793628\n",
      "[12500]\ttraining's binary_logloss: 0.111779\ttraining's amex_metric: 0.971397\tvalid_1's binary_logloss: 0.215995\tvalid_1's amex_metric: 0.793437\n",
      "[13000]\ttraining's binary_logloss: 0.108362\ttraining's amex_metric: 0.974699\tvalid_1's binary_logloss: 0.216085\tvalid_1's amex_metric: 0.793713\n",
      "[13500]\ttraining's binary_logloss: 0.10564\ttraining's amex_metric: 0.978052\tvalid_1's binary_logloss: 0.216109\tvalid_1's amex_metric: 0.793867\n",
      "[14000]\ttraining's binary_logloss: 0.102644\ttraining's amex_metric: 0.980983\tvalid_1's binary_logloss: 0.216232\tvalid_1's amex_metric: 0.793601\n",
      "[14500]\ttraining's binary_logloss: 0.100312\ttraining's amex_metric: 0.983473\tvalid_1's binary_logloss: 0.216309\tvalid_1's amex_metric: 0.793361\n",
      "[15000]\ttraining's binary_logloss: 0.0979459\ttraining's amex_metric: 0.985494\tvalid_1's binary_logloss: 0.216418\tvalid_1's amex_metric: 0.793349\n",
      "Our fold 3 CV score is 0.7933489044880626\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 4 with 1452 features...\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 247023\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 1444\n",
      "[LightGBM] [Info] Using requested OpenCL platform 1 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3090, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 848 dense feature groups (296.90 MB) transferred to GPU in 0.120258 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258935 -> initscore=-1.051512\n",
      "[LightGBM] [Info] Start training from score -1.051512\n",
      "[500]\ttraining's binary_logloss: 0.334598\ttraining's amex_metric: 0.779417\tvalid_1's binary_logloss: 0.33766\tvalid_1's amex_metric: 0.768739\n",
      "[1000]\ttraining's binary_logloss: 0.244582\ttraining's amex_metric: 0.795867\tvalid_1's binary_logloss: 0.25154\tvalid_1's amex_metric: 0.778113\n",
      "[1500]\ttraining's binary_logloss: 0.220951\ttraining's amex_metric: 0.809055\tvalid_1's binary_logloss: 0.23172\tvalid_1's amex_metric: 0.785304\n",
      "[2000]\ttraining's binary_logloss: 0.207561\ttraining's amex_metric: 0.822145\tvalid_1's binary_logloss: 0.223194\tvalid_1's amex_metric: 0.789536\n",
      "[2500]\ttraining's binary_logloss: 0.200676\ttraining's amex_metric: 0.832133\tvalid_1's binary_logloss: 0.220453\tvalid_1's amex_metric: 0.79197\n",
      "[3000]\ttraining's binary_logloss: 0.193748\ttraining's amex_metric: 0.841736\tvalid_1's binary_logloss: 0.218253\tvalid_1's amex_metric: 0.793639\n",
      "[3500]\ttraining's binary_logloss: 0.18731\ttraining's amex_metric: 0.851951\tvalid_1's binary_logloss: 0.216803\tvalid_1's amex_metric: 0.794572\n",
      "[4000]\ttraining's binary_logloss: 0.181641\ttraining's amex_metric: 0.861814\tvalid_1's binary_logloss: 0.215889\tvalid_1's amex_metric: 0.795827\n",
      "[4500]\ttraining's binary_logloss: 0.17608\ttraining's amex_metric: 0.870938\tvalid_1's binary_logloss: 0.21527\tvalid_1's amex_metric: 0.796017\n",
      "[5000]\ttraining's binary_logloss: 0.170586\ttraining's amex_metric: 0.879776\tvalid_1's binary_logloss: 0.21466\tvalid_1's amex_metric: 0.797035\n",
      "[5500]\ttraining's binary_logloss: 0.165696\ttraining's amex_metric: 0.888287\tvalid_1's binary_logloss: 0.214332\tvalid_1's amex_metric: 0.796776\n",
      "[6000]\ttraining's binary_logloss: 0.16149\ttraining's amex_metric: 0.896199\tvalid_1's binary_logloss: 0.214134\tvalid_1's amex_metric: 0.797102\n",
      "[6500]\ttraining's binary_logloss: 0.157095\ttraining's amex_metric: 0.903441\tvalid_1's binary_logloss: 0.213957\tvalid_1's amex_metric: 0.797652\n",
      "[7000]\ttraining's binary_logloss: 0.152012\ttraining's amex_metric: 0.911296\tvalid_1's binary_logloss: 0.213722\tvalid_1's amex_metric: 0.79755\n",
      "[7500]\ttraining's binary_logloss: 0.147194\ttraining's amex_metric: 0.919335\tvalid_1's binary_logloss: 0.213515\tvalid_1's amex_metric: 0.798026\n",
      "[8000]\ttraining's binary_logloss: 0.142901\ttraining's amex_metric: 0.926057\tvalid_1's binary_logloss: 0.213343\tvalid_1's amex_metric: 0.798703\n",
      "[8500]\ttraining's binary_logloss: 0.139226\ttraining's amex_metric: 0.932563\tvalid_1's binary_logloss: 0.213259\tvalid_1's amex_metric: 0.798625\n",
      "[9000]\ttraining's binary_logloss: 0.135092\ttraining's amex_metric: 0.938634\tvalid_1's binary_logloss: 0.213189\tvalid_1's amex_metric: 0.798739\n",
      "[9500]\ttraining's binary_logloss: 0.131382\ttraining's amex_metric: 0.944542\tvalid_1's binary_logloss: 0.213162\tvalid_1's amex_metric: 0.79828\n",
      "[10000]\ttraining's binary_logloss: 0.127747\ttraining's amex_metric: 0.950009\tvalid_1's binary_logloss: 0.213164\tvalid_1's amex_metric: 0.798574\n",
      "[10500]\ttraining's binary_logloss: 0.12457\ttraining's amex_metric: 0.954604\tvalid_1's binary_logloss: 0.213244\tvalid_1's amex_metric: 0.798314\n",
      "[11000]\ttraining's binary_logloss: 0.12133\ttraining's amex_metric: 0.95882\tvalid_1's binary_logloss: 0.213237\tvalid_1's amex_metric: 0.797959\n",
      "[11500]\ttraining's binary_logloss: 0.117949\ttraining's amex_metric: 0.963616\tvalid_1's binary_logloss: 0.213204\tvalid_1's amex_metric: 0.798652\n",
      "[12000]\ttraining's binary_logloss: 0.11488\ttraining's amex_metric: 0.967462\tvalid_1's binary_logloss: 0.213206\tvalid_1's amex_metric: 0.798199\n",
      "[12500]\ttraining's binary_logloss: 0.112209\ttraining's amex_metric: 0.970738\tvalid_1's binary_logloss: 0.213264\tvalid_1's amex_metric: 0.798354\n",
      "[13000]\ttraining's binary_logloss: 0.108797\ttraining's amex_metric: 0.974358\tvalid_1's binary_logloss: 0.213285\tvalid_1's amex_metric: 0.79811\n",
      "[13500]\ttraining's binary_logloss: 0.106109\ttraining's amex_metric: 0.977638\tvalid_1's binary_logloss: 0.213334\tvalid_1's amex_metric: 0.798145\n",
      "[14000]\ttraining's binary_logloss: 0.103109\ttraining's amex_metric: 0.980528\tvalid_1's binary_logloss: 0.213403\tvalid_1's amex_metric: 0.798042\n",
      "[14500]\ttraining's binary_logloss: 0.100768\ttraining's amex_metric: 0.982762\tvalid_1's binary_logloss: 0.213477\tvalid_1's amex_metric: 0.798242\n",
      "[15000]\ttraining's binary_logloss: 0.0983916\ttraining's amex_metric: 0.985123\tvalid_1's binary_logloss: 0.213529\tvalid_1's amex_metric: 0.7982\n",
      "Our fold 4 CV score is 0.7982003195816777\n",
      "Our out of folds CV score is 0.7969041099754193\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ====================================================\n",
    "# Amex metric\n",
    "# ====================================================\n",
    "def amex_metric(y_true, y_pred):\n",
    "    labels = np.transpose(np.array([y_true, y_pred]))\n",
    "    labels = labels[labels[:, 1].argsort()[::-1]]\n",
    "    weights = np.where(labels[:,0]==0, 20, 1)\n",
    "    cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
    "    top_four = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
    "    gini = [0,0]\n",
    "    for i in [1,0]:\n",
    "        labels = np.transpose(np.array([y_true, y_pred]))\n",
    "        labels = labels[labels[:, i].argsort()[::-1]]\n",
    "        weight = np.where(labels[:,0]==0, 20, 1)\n",
    "        weight_random = np.cumsum(weight / np.sum(weight))\n",
    "        total_pos = np.sum(labels[:, 0] *  weight)\n",
    "        cum_pos_found = np.cumsum(labels[:, 0] * weight)\n",
    "        lorentz = cum_pos_found / total_pos\n",
    "        gini[i] = np.sum((lorentz - weight_random) * weight)\n",
    "    return 0.5 * (gini[1]/gini[0] + top_four)\n",
    "\n",
    "# ====================================================\n",
    "# LGBM amex metric\n",
    "# ====================================================\n",
    "def lgb_amex_metric(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    return 'amex_metric', amex_metric(y_true, y_pred), True\n",
    "\n",
    "# ====================================================\n",
    "# Train & Evaluate\n",
    "# ====================================================\n",
    "def train_and_evaluate(train, test):\n",
    "    # Label encode categorical features\n",
    "    cat_features = [\n",
    "        \"B_30\",\n",
    "        \"B_38\",\n",
    "        \"D_114\",\n",
    "        \"D_116\",\n",
    "        \"D_117\",\n",
    "        \"D_120\",\n",
    "        \"D_126\",\n",
    "        \"D_63\",\n",
    "        \"D_64\",\n",
    "        \"D_66\",\n",
    "        \"D_68\"\n",
    "    ]\n",
    "    cat_features = [f\"{cf}_last\" for cf in cat_features]\n",
    "    for cat_col in cat_features:\n",
    "        encoder = LabelEncoder()\n",
    "        train[cat_col] = encoder.fit_transform(train[cat_col])\n",
    "        test[cat_col] = encoder.transform(test[cat_col])\n",
    "    # Round last float features to 2 decimal place\n",
    "    num_cols = list(train.dtypes[(train.dtypes == 'float32') | (train.dtypes == 'float64')].index)\n",
    "    num_cols = [col for col in num_cols if 'last' in col]\n",
    "    for col in num_cols:\n",
    "        train[col + '_round2'] = train[col].round(2)\n",
    "        test[col + '_round2'] = test[col].round(2)\n",
    "    # Get the difference between last and mean\n",
    "    num_cols = [col for col in train.columns if 'last' in col]\n",
    "    num_cols = [col[:-5] for col in num_cols if 'round' not in col]\n",
    "    for col in num_cols:\n",
    "        try:\n",
    "            train[f'{col}_last_mean_diff'] = train[f'{col}_last'] - train[f'{col}_mean']\n",
    "            test[f'{col}_last_mean_diff'] = test[f'{col}_last'] - test[f'{col}_mean']\n",
    "        except:\n",
    "            pass\n",
    "    # Transform float64 and float32 to float16\n",
    "    num_cols = list(train.dtypes[(train.dtypes == 'float32') | (train.dtypes == 'float64')].index)\n",
    "    for col in tqdm(num_cols):\n",
    "        train[col] = train[col].astype(np.float16)\n",
    "        test[col] = test[col].astype(np.float16)\n",
    "    # Get feature list\n",
    "    features = [col for col in train.columns if col not in ['customer_ID', CFG.target]]\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': CFG.metric,\n",
    "        'boosting': CFG.boosting_type,\n",
    "        'seed': CFG.seed,\n",
    "        'num_leaves': 100,\n",
    "        'learning_rate': 0.01,\n",
    "        'feature_fraction': 0.20,\n",
    "        'bagging_freq': 10,\n",
    "        'bagging_fraction': 0.50,\n",
    "        'n_jobs': -1,\n",
    "        'lambda_l2': 2,\n",
    "        #'path_smooth': 20, #added new\n",
    "        'min_data_in_leaf': 40, #changed from 40\n",
    "        'device': 'gpu',\n",
    "        'gpu_platform_id': 1,\n",
    "        'gpu_device_id': 0\n",
    "        }\n",
    "    # Create a numpy array to store test predictions\n",
    "    test_predictions = np.zeros(len(test))\n",
    "    # Create a numpy array to store out of folds predictions\n",
    "    oof_predictions = np.zeros(len(train))\n",
    "    kfold = StratifiedKFold(n_splits = CFG.n_folds, shuffle = True, random_state = CFG.seed)\n",
    "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train, train[CFG.target])):\n",
    "        print(' ')\n",
    "        print('-'*50)\n",
    "        print(f'Training fold {fold} with {len(features)} features...')\n",
    "        x_train, x_val = train[features].iloc[trn_ind], train[features].iloc[val_ind]\n",
    "        y_train, y_val = train[CFG.target].iloc[trn_ind], train[CFG.target].iloc[val_ind]\n",
    "        lgb_train = lgb.Dataset(x_train, y_train, categorical_feature = cat_features)\n",
    "        lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature = cat_features)\n",
    "        model = lgb.train(\n",
    "            params = params,\n",
    "            train_set = lgb_train,\n",
    "            num_boost_round = 15000,\n",
    "            valid_sets = [lgb_train, lgb_valid],\n",
    "            early_stopping_rounds = 1500,\n",
    "            verbose_eval = 500,\n",
    "            feval = lgb_amex_metric\n",
    "            )\n",
    "        # Save best model\n",
    "        joblib.dump(model, OUTPUT_DIR+f'lgbm_fold{fold}_seed{CFG.seed}.pkl')\n",
    "        # Predict validation\n",
    "        val_pred = model.predict(x_val)\n",
    "        # Add to out of folds array\n",
    "        oof_predictions[val_ind] = val_pred\n",
    "        # Predict the test set\n",
    "        test_pred = model.predict(test[features])\n",
    "        test_predictions += test_pred / CFG.n_folds\n",
    "        # Compute fold metric\n",
    "        score = amex_metric(y_val, val_pred)\n",
    "        print(f'Our fold {fold} CV score is {score}')\n",
    "        del x_train, x_val, y_train, y_val, lgb_train, lgb_valid\n",
    "        gc.collect()\n",
    "    # Compute out of folds metric\n",
    "    score = amex_metric(train[CFG.target], oof_predictions)\n",
    "    print(f'Our out of folds CV score is {score}')\n",
    "    # Create a dataframe to store out of folds predictions\n",
    "    oof_df = pd.DataFrame({'customer_ID': train['customer_ID'], 'target': train[CFG.target], 'prediction': oof_predictions})\n",
    "    oof_df.to_csv(OUTPUT_DIR+f'oof_{expt_name}_seed{CFG.seed}.csv', index = False)\n",
    "    # Create a dataframe to store test prediction\n",
    "    test_df = pd.DataFrame({'customer_ID': test['customer_ID'], 'prediction': test_predictions})\n",
    "    test_df.to_csv(OUTPUT_DIR+f'test_{expt_name}_seed{CFG.seed}.csv', index = False)\n",
    "\n",
    "\n",
    "seed_everything(CFG.seed)\n",
    "#train, test = read_data()\n",
    "train_and_evaluate(train, test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}