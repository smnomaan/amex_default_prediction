{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import random\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import itertools\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from itertools import combinations\n",
    "import itertools"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "train = pd.read_parquet('../input/train_fe.parquet')\n",
    "test = pd.read_parquet('../input/test_fe.parquet')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "train = train[['customer_ID', 'P_2_last', 'D_39_last', 'target']] #, 'P_2_last', 'D_39_last'\n",
    "test = test[['customer_ID', 'P_2_last', 'D_39_last']]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "                                         customer_ID  P_2_last  D_39_last\n0  00000469ba478561f23a92a868bd366de6f6527a684c9a...  0.568930          4\n1  00001bf2e77ff879fab36aa4fac689b9ba411dae63ae39...  0.841177          4\n2  0000210045da4f81e5f122c6bde5c2a617d03eef67f82c...  0.697522          0\n3  00003b41e58ede33b8daf61ab56d9952f17c9ad1c3976c...  0.513186         11\n4  00004b22eaeeeb0ec976890c1d9bfc14fd9427e98c4ee9...  0.254478         26",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>customer_ID</th>\n      <th>P_2_last</th>\n      <th>D_39_last</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00000469ba478561f23a92a868bd366de6f6527a684c9a...</td>\n      <td>0.568930</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>00001bf2e77ff879fab36aa4fac689b9ba411dae63ae39...</td>\n      <td>0.841177</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0000210045da4f81e5f122c6bde5c2a617d03eef67f82c...</td>\n      <td>0.697522</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00003b41e58ede33b8daf61ab56d9952f17c9ad1c3976c...</td>\n      <td>0.513186</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00004b22eaeeeb0ec976890c1d9bfc14fd9427e98c4ee9...</td>\n      <td>0.254478</td>\n      <td>26</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Metric:\n",
    "\n",
    "def amex_metric(y_true, y_pred):\n",
    "    labels = np.transpose(np.array([y_true, y_pred]))\n",
    "    labels = labels[labels[:, 1].argsort()[::-1]]\n",
    "    weights = np.where(labels[:,0]==0, 20, 1)\n",
    "    cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
    "    top_four = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
    "    gini = [0,0]\n",
    "    for i in [1,0]:\n",
    "        labels = np.transpose(np.array([y_true, y_pred]))\n",
    "        labels = labels[labels[:, i].argsort()[::-1]]\n",
    "        weight = np.where(labels[:,0]==0, 20, 1)\n",
    "        weight_random = np.cumsum(weight / np.sum(weight))\n",
    "        total_pos = np.sum(labels[:, 0] *  weight)\n",
    "        cum_pos_found = np.cumsum(labels[:, 0] * weight)\n",
    "        lorentz = cum_pos_found / total_pos\n",
    "        gini[i] = np.sum((lorentz - weight_random) * weight)\n",
    "    return 0.5 * (gini[1]/gini[0] + top_four)\n",
    "\n",
    "# ====================================================\n",
    "# LGBM amex metric\n",
    "# ====================================================\n",
    "def lgb_amex_metric(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    return 'amex_metric', amex_metric(y_true, y_pred), True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "oof_new = pd.read_csv('../lgbm_new_5_fold/oof_lgbm_new_5_fold_seed42.csv')\n",
    "oof_smn = pd.read_csv('../lgbm_smn_fold/oof_lgbm_smn_fold_seed42.csv')\n",
    "#oof_lag = pd.read_csv('../lgbm_lag_5_fold/oof_lgbm_lag_5_fold_5fold_seed42.csv')\n",
    "oof_xgb = pd.read_csv('../xgb_fe_8_fold/oof_xgb_fe_8_fold_seed42.csv')\n",
    "oof_xgb_smn = pd.read_csv('../xgb_smn_5_fold/oof_xgb_smn_5_fold_seed42.csv')\n",
    "#oof_bobw = pd.read_csv('../xgb_BOBW_5_fold/oof_xgb_BOBW_5_fold_seed42.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "train['pred_new'] = oof_new.prediction\n",
    "train['pred_smn'] = oof_smn.prediction\n",
    "#train['pred_lag'] = oof_lag.prediction\n",
    "train['pred_xgb'] = oof_xgb.prediction\n",
    "train['pred_xgb_smn'] = oof_xgb_smn.prediction\n",
    "#train['pred_xgb_bobw'] = oof_bobw.prediction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "train = train[['customer_ID' , 'P_2_last', 'D_39_last', 'pred_new', 'pred_smn', 'pred_xgb','pred_xgb_smn', 'target']] #'pred_lag',  'pred_xgb_bobw',"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "                                         customer_ID  P_2_last  D_39_last  pred_new  pred_smn  pred_xgb  pred_xgb_smn  target\n0  0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...  0.934745          0  0.000307  0.000128  0.000262      0.000162       0\n1  00000fd6641609c6ece5454664794f0340ad84dddce9a2...  0.880519          6  0.000957  0.000704  0.001429      0.001026       0\n2  00001b22f846c82c51f6e3958ccd81970162bae8b007e8...  0.880875          0  0.001072  0.001070  0.001446      0.001510       0\n3  000041bdba6ecadd89a52d11886e8eaaec9325906c9723...  0.621776          0  0.003317  0.001992  0.008611      0.003684       0\n4  00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8a...  0.871900          0  0.001673  0.000634  0.000680      0.001328       0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>customer_ID</th>\n      <th>P_2_last</th>\n      <th>D_39_last</th>\n      <th>pred_new</th>\n      <th>pred_smn</th>\n      <th>pred_xgb</th>\n      <th>pred_xgb_smn</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...</td>\n      <td>0.934745</td>\n      <td>0</td>\n      <td>0.000307</td>\n      <td>0.000128</td>\n      <td>0.000262</td>\n      <td>0.000162</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>00000fd6641609c6ece5454664794f0340ad84dddce9a2...</td>\n      <td>0.880519</td>\n      <td>6</td>\n      <td>0.000957</td>\n      <td>0.000704</td>\n      <td>0.001429</td>\n      <td>0.001026</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00001b22f846c82c51f6e3958ccd81970162bae8b007e8...</td>\n      <td>0.880875</td>\n      <td>0</td>\n      <td>0.001072</td>\n      <td>0.001070</td>\n      <td>0.001446</td>\n      <td>0.001510</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>000041bdba6ecadd89a52d11886e8eaaec9325906c9723...</td>\n      <td>0.621776</td>\n      <td>0</td>\n      <td>0.003317</td>\n      <td>0.001992</td>\n      <td>0.008611</td>\n      <td>0.003684</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8a...</td>\n      <td>0.871900</td>\n      <td>0</td>\n      <td>0.001673</td>\n      <td>0.000634</td>\n      <td>0.000680</td>\n      <td>0.001328</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "test_new = pd.read_csv('../lgbm_new_5_fold/test_lgbm_new_5_fold_seed42.csv')\n",
    "test_smn = pd.read_csv('../lgbm_smn_fold/test_lgbm_smn_fold_seed42.csv')\n",
    "#test_lag = pd.read_csv('../lgbm_lag_5_fold/test_lgbm_lag_5_fold_5fold_seed42.csv')\n",
    "test_xgb = pd.read_csv('../xgb_fe_8_fold/test_xgb_fe_8_fold_seed42.csv')\n",
    "test_xgb_smn = pd.read_csv('../xgb_smn_5_fold/test_xgb_smn_5_fold_seed42.csv')\n",
    "#test_xgb_bobw = pd.read_csv('../xgb_BOBW_5_fold/test_xgb_BOBW_5_fold_seed42.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "test['pred_new'] = test_new.prediction\n",
    "test['pred_smn'] = test_smn.prediction\n",
    "#test['pred_lag'] = test_lag.prediction\n",
    "test['pred_xgb'] = test_xgb.prediction\n",
    "test['pred_xgb_smn'] = test_xgb_smn.prediction\n",
    "#test['pred_xgb_bobw'] = test_xgb_bobw.prediction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "KstestResult(statistic=0.057840996473149525, pvalue=0.0)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import ks_2samp\n",
    "\n",
    "#perform Kolmogorov-Smirnov test\n",
    "ks_2samp(test_xgb.prediction, test_new.prediction)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "                                         customer_ID  P_2_last  D_39_last  pred_new  pred_smn  pred_xgb  pred_xgb_smn\n0  00000469ba478561f23a92a868bd366de6f6527a684c9a...  0.568930          4  0.029524  0.028134  0.032877      0.024319\n1  00001bf2e77ff879fab36aa4fac689b9ba411dae63ae39...  0.841177          4  0.001012  0.000574  0.000531      0.000385\n2  0000210045da4f81e5f122c6bde5c2a617d03eef67f82c...  0.697522          0  0.038608  0.025484  0.033457      0.027844\n3  00003b41e58ede33b8daf61ab56d9952f17c9ad1c3976c...  0.513186         11  0.179223  0.200430  0.153475      0.156202\n4  00004b22eaeeeb0ec976890c1d9bfc14fd9427e98c4ee9...  0.254478         26  0.869733  0.905022  0.920015      0.925976",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>customer_ID</th>\n      <th>P_2_last</th>\n      <th>D_39_last</th>\n      <th>pred_new</th>\n      <th>pred_smn</th>\n      <th>pred_xgb</th>\n      <th>pred_xgb_smn</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00000469ba478561f23a92a868bd366de6f6527a684c9a...</td>\n      <td>0.568930</td>\n      <td>4</td>\n      <td>0.029524</td>\n      <td>0.028134</td>\n      <td>0.032877</td>\n      <td>0.024319</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>00001bf2e77ff879fab36aa4fac689b9ba411dae63ae39...</td>\n      <td>0.841177</td>\n      <td>4</td>\n      <td>0.001012</td>\n      <td>0.000574</td>\n      <td>0.000531</td>\n      <td>0.000385</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0000210045da4f81e5f122c6bde5c2a617d03eef67f82c...</td>\n      <td>0.697522</td>\n      <td>0</td>\n      <td>0.038608</td>\n      <td>0.025484</td>\n      <td>0.033457</td>\n      <td>0.027844</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00003b41e58ede33b8daf61ab56d9952f17c9ad1c3976c...</td>\n      <td>0.513186</td>\n      <td>11</td>\n      <td>0.179223</td>\n      <td>0.200430</td>\n      <td>0.153475</td>\n      <td>0.156202</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00004b22eaeeeb0ec976890c1d9bfc14fd9427e98c4ee9...</td>\n      <td>0.254478</td>\n      <td>26</td>\n      <td>0.869733</td>\n      <td>0.905022</td>\n      <td>0.920015</td>\n      <td>0.925976</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "\n",
    "# ====================================================\n",
    "# Configurations\n",
    "# ====================================================\n",
    "class CFG:\n",
    "    input_dir = '../input/'\n",
    "    seed = 42\n",
    "    n_folds = 5\n",
    "    target = 'target'\n",
    "    boosting_type = 'dart'\n",
    "    metric = 'binary_logloss'\n",
    "\n",
    "# ====================================================\n",
    "# Seed everything\n",
    "# ====================================================\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Directory settings\n",
    "# ====================================================\n",
    "import os\n",
    "\n",
    "expt_name = \"stacking_lgbm_6\"\n",
    "OUTPUT_DIR = f'../{expt_name}/'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "--------------------------------------------------\n",
      "Training fold 0 with 6 features...\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Info] Total Bins 1391\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "[100]\ttraining's binary_logloss: 0.451621\ttraining's amex_metric: 0.795514\tvalid_1's binary_logloss: 0.450765\tvalid_1's amex_metric: 0.801315\n",
      "[200]\ttraining's binary_logloss: 0.429189\ttraining's amex_metric: 0.795818\tvalid_1's binary_logloss: 0.428083\tvalid_1's amex_metric: 0.802361\n",
      "[300]\ttraining's binary_logloss: 0.372638\ttraining's amex_metric: 0.795987\tvalid_1's binary_logloss: 0.371162\tvalid_1's amex_metric: 0.803498\n",
      "[400]\ttraining's binary_logloss: 0.346793\ttraining's amex_metric: 0.796171\tvalid_1's binary_logloss: 0.345146\tvalid_1's amex_metric: 0.802958\n",
      "[500]\ttraining's binary_logloss: 0.315559\ttraining's amex_metric: 0.796481\tvalid_1's binary_logloss: 0.31373\tvalid_1's amex_metric: 0.803634\n",
      "[600]\ttraining's binary_logloss: 0.29751\ttraining's amex_metric: 0.796561\tvalid_1's binary_logloss: 0.295568\tvalid_1's amex_metric: 0.803476\n",
      "[700]\ttraining's binary_logloss: 0.269375\ttraining's amex_metric: 0.796657\tvalid_1's binary_logloss: 0.267233\tvalid_1's amex_metric: 0.803719\n",
      "[800]\ttraining's binary_logloss: 0.251057\ttraining's amex_metric: 0.796909\tvalid_1's binary_logloss: 0.248709\tvalid_1's amex_metric: 0.803729\n",
      "[900]\ttraining's binary_logloss: 0.238516\ttraining's amex_metric: 0.797007\tvalid_1's binary_logloss: 0.236022\tvalid_1's amex_metric: 0.804084\n",
      "[1000]\ttraining's binary_logloss: 0.233624\ttraining's amex_metric: 0.797054\tvalid_1's binary_logloss: 0.23107\tvalid_1's amex_metric: 0.80419\n",
      "[1100]\ttraining's binary_logloss: 0.226548\ttraining's amex_metric: 0.797085\tvalid_1's binary_logloss: 0.223897\tvalid_1's amex_metric: 0.804033\n",
      "[1200]\ttraining's binary_logloss: 0.224682\ttraining's amex_metric: 0.797104\tvalid_1's binary_logloss: 0.222011\tvalid_1's amex_metric: 0.803925\n",
      "[1300]\ttraining's binary_logloss: 0.22241\ttraining's amex_metric: 0.797079\tvalid_1's binary_logloss: 0.219728\tvalid_1's amex_metric: 0.803947\n",
      "[1400]\ttraining's binary_logloss: 0.219956\ttraining's amex_metric: 0.796973\tvalid_1's binary_logloss: 0.217262\tvalid_1's amex_metric: 0.804031\n",
      "[1500]\ttraining's binary_logloss: 0.219071\ttraining's amex_metric: 0.797166\tvalid_1's binary_logloss: 0.216399\tvalid_1's amex_metric: 0.804046\n",
      "[1600]\ttraining's binary_logloss: 0.216904\ttraining's amex_metric: 0.797038\tvalid_1's binary_logloss: 0.214223\tvalid_1's amex_metric: 0.8038\n",
      "[1700]\ttraining's binary_logloss: 0.21626\ttraining's amex_metric: 0.797126\tvalid_1's binary_logloss: 0.213595\tvalid_1's amex_metric: 0.804062\n",
      "[1800]\ttraining's binary_logloss: 0.216115\ttraining's amex_metric: 0.797156\tvalid_1's binary_logloss: 0.213467\tvalid_1's amex_metric: 0.804158\n",
      "[1900]\ttraining's binary_logloss: 0.215601\ttraining's amex_metric: 0.797172\tvalid_1's binary_logloss: 0.212964\tvalid_1's amex_metric: 0.804172\n",
      "[2000]\ttraining's binary_logloss: 0.21511\ttraining's amex_metric: 0.797176\tvalid_1's binary_logloss: 0.212487\tvalid_1's amex_metric: 0.803964\n",
      "[2100]\ttraining's binary_logloss: 0.214989\ttraining's amex_metric: 0.797095\tvalid_1's binary_logloss: 0.212388\tvalid_1's amex_metric: 0.80421\n",
      "[2200]\ttraining's binary_logloss: 0.214895\ttraining's amex_metric: 0.797077\tvalid_1's binary_logloss: 0.21231\tvalid_1's amex_metric: 0.804081\n",
      "[2300]\ttraining's binary_logloss: 0.214636\ttraining's amex_metric: 0.797006\tvalid_1's binary_logloss: 0.212061\tvalid_1's amex_metric: 0.803916\n",
      "[2400]\ttraining's binary_logloss: 0.214602\ttraining's amex_metric: 0.797134\tvalid_1's binary_logloss: 0.212046\tvalid_1's amex_metric: 0.803942\n",
      "[2500]\ttraining's binary_logloss: 0.21454\ttraining's amex_metric: 0.797165\tvalid_1's binary_logloss: 0.212\tvalid_1's amex_metric: 0.803901\n",
      "[2600]\ttraining's binary_logloss: 0.214448\ttraining's amex_metric: 0.797198\tvalid_1's binary_logloss: 0.211927\tvalid_1's amex_metric: 0.804105\n",
      "[2700]\ttraining's binary_logloss: 0.214303\ttraining's amex_metric: 0.797197\tvalid_1's binary_logloss: 0.211791\tvalid_1's amex_metric: 0.804\n",
      "[2800]\ttraining's binary_logloss: 0.214339\ttraining's amex_metric: 0.79721\tvalid_1's binary_logloss: 0.211849\tvalid_1's amex_metric: 0.803897\n",
      "[2900]\ttraining's binary_logloss: 0.214192\ttraining's amex_metric: 0.797092\tvalid_1's binary_logloss: 0.21171\tvalid_1's amex_metric: 0.803875\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3000]\ttraining's binary_logloss: 0.214079\ttraining's amex_metric: 0.797151\tvalid_1's binary_logloss: 0.211605\tvalid_1's amex_metric: 0.803755\n",
      "------------fold 0 CV score is 0.8037547388905604--------------\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 1 with 6 features...\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Info] Total Bins 1394\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "[100]\ttraining's binary_logloss: 0.451254\ttraining's amex_metric: 0.797831\tvalid_1's binary_logloss: 0.451588\tvalid_1's amex_metric: 0.792998\n",
      "[200]\ttraining's binary_logloss: 0.428752\ttraining's amex_metric: 0.797902\tvalid_1's binary_logloss: 0.429152\tvalid_1's amex_metric: 0.793262\n",
      "[300]\ttraining's binary_logloss: 0.372094\ttraining's amex_metric: 0.798424\tvalid_1's binary_logloss: 0.372689\tvalid_1's amex_metric: 0.7938\n",
      "[400]\ttraining's binary_logloss: 0.346178\ttraining's amex_metric: 0.798428\tvalid_1's binary_logloss: 0.346912\tvalid_1's amex_metric: 0.793778\n",
      "[500]\ttraining's binary_logloss: 0.314936\ttraining's amex_metric: 0.798965\tvalid_1's binary_logloss: 0.315818\tvalid_1's amex_metric: 0.794213\n",
      "[600]\ttraining's binary_logloss: 0.296803\ttraining's amex_metric: 0.799071\tvalid_1's binary_logloss: 0.297816\tvalid_1's amex_metric: 0.79459\n",
      "[700]\ttraining's binary_logloss: 0.268599\ttraining's amex_metric: 0.799142\tvalid_1's binary_logloss: 0.269721\tvalid_1's amex_metric: 0.794996\n",
      "[800]\ttraining's binary_logloss: 0.250187\ttraining's amex_metric: 0.799208\tvalid_1's binary_logloss: 0.251475\tvalid_1's amex_metric: 0.794584\n",
      "[900]\ttraining's binary_logloss: 0.237591\ttraining's amex_metric: 0.799453\tvalid_1's binary_logloss: 0.23904\tvalid_1's amex_metric: 0.794662\n",
      "[1000]\ttraining's binary_logloss: 0.232721\ttraining's amex_metric: 0.799288\tvalid_1's binary_logloss: 0.234223\tvalid_1's amex_metric: 0.794677\n",
      "[1100]\ttraining's binary_logloss: 0.225611\ttraining's amex_metric: 0.799309\tvalid_1's binary_logloss: 0.227227\tvalid_1's amex_metric: 0.794603\n",
      "[1200]\ttraining's binary_logloss: 0.22372\ttraining's amex_metric: 0.799409\tvalid_1's binary_logloss: 0.225371\tvalid_1's amex_metric: 0.794897\n",
      "[1300]\ttraining's binary_logloss: 0.221469\ttraining's amex_metric: 0.799474\tvalid_1's binary_logloss: 0.223155\tvalid_1's amex_metric: 0.794616\n",
      "[1400]\ttraining's binary_logloss: 0.218999\ttraining's amex_metric: 0.799514\tvalid_1's binary_logloss: 0.220772\tvalid_1's amex_metric: 0.794506\n",
      "[1500]\ttraining's binary_logloss: 0.218102\ttraining's amex_metric: 0.799568\tvalid_1's binary_logloss: 0.219872\tvalid_1's amex_metric: 0.794357\n",
      "[1600]\ttraining's binary_logloss: 0.215933\ttraining's amex_metric: 0.79956\tvalid_1's binary_logloss: 0.217772\tvalid_1's amex_metric: 0.794515\n",
      "[1700]\ttraining's binary_logloss: 0.215277\ttraining's amex_metric: 0.799576\tvalid_1's binary_logloss: 0.217129\tvalid_1's amex_metric: 0.794646\n",
      "[1800]\ttraining's binary_logloss: 0.215139\ttraining's amex_metric: 0.799502\tvalid_1's binary_logloss: 0.216988\tvalid_1's amex_metric: 0.794495\n",
      "[1900]\ttraining's binary_logloss: 0.214639\ttraining's amex_metric: 0.799559\tvalid_1's binary_logloss: 0.216527\tvalid_1's amex_metric: 0.794644\n",
      "[2000]\ttraining's binary_logloss: 0.214146\ttraining's amex_metric: 0.799547\tvalid_1's binary_logloss: 0.216082\tvalid_1's amex_metric: 0.794633\n",
      "[2100]\ttraining's binary_logloss: 0.214002\ttraining's amex_metric: 0.799518\tvalid_1's binary_logloss: 0.215966\tvalid_1's amex_metric: 0.794526\n",
      "[2200]\ttraining's binary_logloss: 0.213921\ttraining's amex_metric: 0.799543\tvalid_1's binary_logloss: 0.215895\tvalid_1's amex_metric: 0.794336\n",
      "[2300]\ttraining's binary_logloss: 0.213658\ttraining's amex_metric: 0.799489\tvalid_1's binary_logloss: 0.215675\tvalid_1's amex_metric: 0.794272\n",
      "[2400]\ttraining's binary_logloss: 0.213639\ttraining's amex_metric: 0.799542\tvalid_1's binary_logloss: 0.215665\tvalid_1's amex_metric: 0.794359\n",
      "[2500]\ttraining's binary_logloss: 0.213576\ttraining's amex_metric: 0.799471\tvalid_1's binary_logloss: 0.215627\tvalid_1's amex_metric: 0.794276\n",
      "[2600]\ttraining's binary_logloss: 0.21348\ttraining's amex_metric: 0.79946\tvalid_1's binary_logloss: 0.215559\tvalid_1's amex_metric: 0.794217\n",
      "[2700]\ttraining's binary_logloss: 0.213361\ttraining's amex_metric: 0.799524\tvalid_1's binary_logloss: 0.215478\tvalid_1's amex_metric: 0.794402\n",
      "[2800]\ttraining's binary_logloss: 0.213386\ttraining's amex_metric: 0.799518\tvalid_1's binary_logloss: 0.215526\tvalid_1's amex_metric: 0.79438\n",
      "[2900]\ttraining's binary_logloss: 0.213245\ttraining's amex_metric: 0.799519\tvalid_1's binary_logloss: 0.215434\tvalid_1's amex_metric: 0.794358\n",
      "[3000]\ttraining's binary_logloss: 0.213131\ttraining's amex_metric: 0.799625\tvalid_1's binary_logloss: 0.215366\tvalid_1's amex_metric: 0.794375\n",
      "------------fold 1 CV score is 0.794375065028736--------------\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 2 with 6 features...\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Info] Total Bins 1391\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "[100]\ttraining's binary_logloss: 0.451294\ttraining's amex_metric: 0.796316\tvalid_1's binary_logloss: 0.451396\tvalid_1's amex_metric: 0.796661\n",
      "[200]\ttraining's binary_logloss: 0.428799\ttraining's amex_metric: 0.797021\tvalid_1's binary_logloss: 0.428872\tvalid_1's amex_metric: 0.797696\n",
      "[300]\ttraining's binary_logloss: 0.372193\ttraining's amex_metric: 0.797431\tvalid_1's binary_logloss: 0.372314\tvalid_1's amex_metric: 0.797738\n",
      "[400]\ttraining's binary_logloss: 0.346377\ttraining's amex_metric: 0.797442\tvalid_1's binary_logloss: 0.346435\tvalid_1's amex_metric: 0.798036\n",
      "[500]\ttraining's binary_logloss: 0.315125\ttraining's amex_metric: 0.797651\tvalid_1's binary_logloss: 0.315231\tvalid_1's amex_metric: 0.798471\n",
      "[600]\ttraining's binary_logloss: 0.297029\ttraining's amex_metric: 0.797913\tvalid_1's binary_logloss: 0.297175\tvalid_1's amex_metric: 0.798679\n",
      "[700]\ttraining's binary_logloss: 0.268806\ttraining's amex_metric: 0.798228\tvalid_1's binary_logloss: 0.268994\tvalid_1's amex_metric: 0.799359\n",
      "[800]\ttraining's binary_logloss: 0.250434\ttraining's amex_metric: 0.798139\tvalid_1's binary_logloss: 0.250649\tvalid_1's amex_metric: 0.799201\n",
      "[900]\ttraining's binary_logloss: 0.23784\ttraining's amex_metric: 0.798201\tvalid_1's binary_logloss: 0.238157\tvalid_1's amex_metric: 0.799233\n",
      "[1000]\ttraining's binary_logloss: 0.232981\ttraining's amex_metric: 0.798364\tvalid_1's binary_logloss: 0.233298\tvalid_1's amex_metric: 0.798941\n",
      "[1100]\ttraining's binary_logloss: 0.225875\ttraining's amex_metric: 0.798443\tvalid_1's binary_logloss: 0.226238\tvalid_1's amex_metric: 0.798832\n",
      "[1200]\ttraining's binary_logloss: 0.223995\ttraining's amex_metric: 0.798364\tvalid_1's binary_logloss: 0.224326\tvalid_1's amex_metric: 0.798677\n",
      "[1300]\ttraining's binary_logloss: 0.221737\ttraining's amex_metric: 0.798539\tvalid_1's binary_logloss: 0.222078\tvalid_1's amex_metric: 0.798404\n",
      "[1400]\ttraining's binary_logloss: 0.219264\ttraining's amex_metric: 0.798515\tvalid_1's binary_logloss: 0.21964\tvalid_1's amex_metric: 0.798171\n",
      "[1500]\ttraining's binary_logloss: 0.218365\ttraining's amex_metric: 0.798544\tvalid_1's binary_logloss: 0.218741\tvalid_1's amex_metric: 0.798743\n",
      "[1600]\ttraining's binary_logloss: 0.216199\ttraining's amex_metric: 0.798498\tvalid_1's binary_logloss: 0.216639\tvalid_1's amex_metric: 0.798761\n",
      "[1700]\ttraining's binary_logloss: 0.215577\ttraining's amex_metric: 0.798636\tvalid_1's binary_logloss: 0.216016\tvalid_1's amex_metric: 0.798782\n",
      "[1800]\ttraining's binary_logloss: 0.21541\ttraining's amex_metric: 0.798605\tvalid_1's binary_logloss: 0.215815\tvalid_1's amex_metric: 0.79893\n",
      "[1900]\ttraining's binary_logloss: 0.214916\ttraining's amex_metric: 0.798638\tvalid_1's binary_logloss: 0.215332\tvalid_1's amex_metric: 0.798974\n",
      "[2000]\ttraining's binary_logloss: 0.214447\ttraining's amex_metric: 0.79857\tvalid_1's binary_logloss: 0.214885\tvalid_1's amex_metric: 0.799186\n",
      "[2100]\ttraining's binary_logloss: 0.214313\ttraining's amex_metric: 0.798357\tvalid_1's binary_logloss: 0.214753\tvalid_1's amex_metric: 0.798875\n",
      "[2200]\ttraining's binary_logloss: 0.21424\ttraining's amex_metric: 0.798469\tvalid_1's binary_logloss: 0.214684\tvalid_1's amex_metric: 0.799274\n",
      "[2300]\ttraining's binary_logloss: 0.213991\ttraining's amex_metric: 0.798502\tvalid_1's binary_logloss: 0.214458\tvalid_1's amex_metric: 0.799278\n",
      "[2400]\ttraining's binary_logloss: 0.213957\ttraining's amex_metric: 0.798362\tvalid_1's binary_logloss: 0.214434\tvalid_1's amex_metric: 0.799046\n",
      "[2500]\ttraining's binary_logloss: 0.2139\ttraining's amex_metric: 0.79843\tvalid_1's binary_logloss: 0.214391\tvalid_1's amex_metric: 0.799169\n",
      "[2600]\ttraining's binary_logloss: 0.213796\ttraining's amex_metric: 0.798508\tvalid_1's binary_logloss: 0.214311\tvalid_1's amex_metric: 0.799232\n",
      "[2700]\ttraining's binary_logloss: 0.213652\ttraining's amex_metric: 0.798518\tvalid_1's binary_logloss: 0.214191\tvalid_1's amex_metric: 0.799236\n",
      "[2800]\ttraining's binary_logloss: 0.213692\ttraining's amex_metric: 0.798532\tvalid_1's binary_logloss: 0.214234\tvalid_1's amex_metric: 0.799318\n",
      "[2900]\ttraining's binary_logloss: 0.21355\ttraining's amex_metric: 0.798642\tvalid_1's binary_logloss: 0.21413\tvalid_1's amex_metric: 0.799528\n",
      "[3000]\ttraining's binary_logloss: 0.213445\ttraining's amex_metric: 0.798697\tvalid_1's binary_logloss: 0.214062\tvalid_1's amex_metric: 0.799544\n",
      "------------fold 2 CV score is 0.7995436950856176--------------\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 3 with 6 features...\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Info] Total Bins 1393\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258935 -> initscore=-1.051512\n",
      "[LightGBM] [Info] Start training from score -1.051512\n",
      "[100]\ttraining's binary_logloss: 0.45136\ttraining's amex_metric: 0.797838\tvalid_1's binary_logloss: 0.452056\tvalid_1's amex_metric: 0.792256\n",
      "[200]\ttraining's binary_logloss: 0.428739\ttraining's amex_metric: 0.798552\tvalid_1's binary_logloss: 0.429654\tvalid_1's amex_metric: 0.792413\n",
      "[300]\ttraining's binary_logloss: 0.372013\ttraining's amex_metric: 0.79852\tvalid_1's binary_logloss: 0.373185\tvalid_1's amex_metric: 0.792501\n",
      "[400]\ttraining's binary_logloss: 0.346201\ttraining's amex_metric: 0.798909\tvalid_1's binary_logloss: 0.347476\tvalid_1's amex_metric: 0.792728\n",
      "[500]\ttraining's binary_logloss: 0.314881\ttraining's amex_metric: 0.799145\tvalid_1's binary_logloss: 0.316303\tvalid_1's amex_metric: 0.792557\n",
      "[600]\ttraining's binary_logloss: 0.296762\ttraining's amex_metric: 0.799342\tvalid_1's binary_logloss: 0.298323\tvalid_1's amex_metric: 0.792698\n",
      "[700]\ttraining's binary_logloss: 0.268552\ttraining's amex_metric: 0.799515\tvalid_1's binary_logloss: 0.270319\tvalid_1's amex_metric: 0.793374\n",
      "[800]\ttraining's binary_logloss: 0.250158\ttraining's amex_metric: 0.799678\tvalid_1's binary_logloss: 0.252036\tvalid_1's amex_metric: 0.793584\n",
      "[900]\ttraining's binary_logloss: 0.237564\ttraining's amex_metric: 0.799604\tvalid_1's binary_logloss: 0.239525\tvalid_1's amex_metric: 0.79374\n",
      "[1000]\ttraining's binary_logloss: 0.232651\ttraining's amex_metric: 0.799629\tvalid_1's binary_logloss: 0.23469\tvalid_1's amex_metric: 0.793933\n",
      "[1100]\ttraining's binary_logloss: 0.225546\ttraining's amex_metric: 0.799755\tvalid_1's binary_logloss: 0.227693\tvalid_1's amex_metric: 0.794144\n",
      "[1200]\ttraining's binary_logloss: 0.223674\ttraining's amex_metric: 0.799781\tvalid_1's binary_logloss: 0.225863\tvalid_1's amex_metric: 0.794231\n",
      "[1300]\ttraining's binary_logloss: 0.221405\ttraining's amex_metric: 0.799788\tvalid_1's binary_logloss: 0.223649\tvalid_1's amex_metric: 0.794089\n",
      "[1400]\ttraining's binary_logloss: 0.218926\ttraining's amex_metric: 0.799815\tvalid_1's binary_logloss: 0.221228\tvalid_1's amex_metric: 0.79416\n",
      "[1500]\ttraining's binary_logloss: 0.218021\ttraining's amex_metric: 0.79974\tvalid_1's binary_logloss: 0.22035\tvalid_1's amex_metric: 0.7942\n",
      "[1600]\ttraining's binary_logloss: 0.215845\ttraining's amex_metric: 0.799852\tvalid_1's binary_logloss: 0.218206\tvalid_1's amex_metric: 0.794167\n",
      "[1700]\ttraining's binary_logloss: 0.215222\ttraining's amex_metric: 0.799764\tvalid_1's binary_logloss: 0.217622\tvalid_1's amex_metric: 0.794356\n",
      "[1800]\ttraining's binary_logloss: 0.21507\ttraining's amex_metric: 0.799755\tvalid_1's binary_logloss: 0.217484\tvalid_1's amex_metric: 0.794214\n",
      "[1900]\ttraining's binary_logloss: 0.214583\ttraining's amex_metric: 0.799719\tvalid_1's binary_logloss: 0.217011\tvalid_1's amex_metric: 0.794492\n",
      "[2000]\ttraining's binary_logloss: 0.214074\ttraining's amex_metric: 0.799802\tvalid_1's binary_logloss: 0.216528\tvalid_1's amex_metric: 0.794566\n",
      "[2100]\ttraining's binary_logloss: 0.213942\ttraining's amex_metric: 0.799908\tvalid_1's binary_logloss: 0.216418\tvalid_1's amex_metric: 0.794174\n",
      "[2200]\ttraining's binary_logloss: 0.213862\ttraining's amex_metric: 0.799859\tvalid_1's binary_logloss: 0.216351\tvalid_1's amex_metric: 0.794073\n",
      "[2300]\ttraining's binary_logloss: 0.213598\ttraining's amex_metric: 0.799888\tvalid_1's binary_logloss: 0.216111\tvalid_1's amex_metric: 0.794434\n",
      "[2400]\ttraining's binary_logloss: 0.213572\ttraining's amex_metric: 0.799778\tvalid_1's binary_logloss: 0.216099\tvalid_1's amex_metric: 0.79446\n",
      "[2500]\ttraining's binary_logloss: 0.213514\ttraining's amex_metric: 0.799924\tvalid_1's binary_logloss: 0.216052\tvalid_1's amex_metric: 0.794492\n",
      "[2600]\ttraining's binary_logloss: 0.213421\ttraining's amex_metric: 0.799975\tvalid_1's binary_logloss: 0.215968\tvalid_1's amex_metric: 0.794623\n",
      "[2700]\ttraining's binary_logloss: 0.213284\ttraining's amex_metric: 0.799944\tvalid_1's binary_logloss: 0.215843\tvalid_1's amex_metric: 0.794621\n",
      "[2800]\ttraining's binary_logloss: 0.213309\ttraining's amex_metric: 0.799921\tvalid_1's binary_logloss: 0.215867\tvalid_1's amex_metric: 0.794497\n",
      "[2900]\ttraining's binary_logloss: 0.213172\ttraining's amex_metric: 0.79997\tvalid_1's binary_logloss: 0.215741\tvalid_1's amex_metric: 0.794669\n",
      "[3000]\ttraining's binary_logloss: 0.213074\ttraining's amex_metric: 0.799923\tvalid_1's binary_logloss: 0.215667\tvalid_1's amex_metric: 0.794649\n",
      "------------fold 3 CV score is 0.7946493984020329--------------\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 4 with 6 features...\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Info] Total Bins 1390\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258935 -> initscore=-1.051512\n",
      "[LightGBM] [Info] Start training from score -1.051512\n",
      "[100]\ttraining's binary_logloss: 0.451535\ttraining's amex_metric: 0.796815\tvalid_1's binary_logloss: 0.4516\tvalid_1's amex_metric: 0.797486\n",
      "[200]\ttraining's binary_logloss: 0.428934\ttraining's amex_metric: 0.796778\tvalid_1's binary_logloss: 0.429127\tvalid_1's amex_metric: 0.797202\n",
      "[300]\ttraining's binary_logloss: 0.372389\ttraining's amex_metric: 0.797074\tvalid_1's binary_logloss: 0.372622\tvalid_1's amex_metric: 0.79734\n",
      "[400]\ttraining's binary_logloss: 0.346543\ttraining's amex_metric: 0.797444\tvalid_1's binary_logloss: 0.346766\tvalid_1's amex_metric: 0.798289\n",
      "[500]\ttraining's binary_logloss: 0.315285\ttraining's amex_metric: 0.797744\tvalid_1's binary_logloss: 0.315544\tvalid_1's amex_metric: 0.798353\n",
      "[600]\ttraining's binary_logloss: 0.297237\ttraining's amex_metric: 0.797712\tvalid_1's binary_logloss: 0.297471\tvalid_1's amex_metric: 0.798291\n",
      "[700]\ttraining's binary_logloss: 0.269016\ttraining's amex_metric: 0.797936\tvalid_1's binary_logloss: 0.269222\tvalid_1's amex_metric: 0.798182\n",
      "[800]\ttraining's binary_logloss: 0.250674\ttraining's amex_metric: 0.798378\tvalid_1's binary_logloss: 0.250808\tvalid_1's amex_metric: 0.798089\n",
      "[900]\ttraining's binary_logloss: 0.238113\ttraining's amex_metric: 0.79815\tvalid_1's binary_logloss: 0.238175\tvalid_1's amex_metric: 0.798238\n",
      "[1000]\ttraining's binary_logloss: 0.233245\ttraining's amex_metric: 0.798203\tvalid_1's binary_logloss: 0.233281\tvalid_1's amex_metric: 0.798605\n",
      "[1100]\ttraining's binary_logloss: 0.226131\ttraining's amex_metric: 0.798105\tvalid_1's binary_logloss: 0.226062\tvalid_1's amex_metric: 0.798615\n",
      "[1200]\ttraining's binary_logloss: 0.224268\ttraining's amex_metric: 0.798248\tvalid_1's binary_logloss: 0.224187\tvalid_1's amex_metric: 0.798723\n",
      "[1300]\ttraining's binary_logloss: 0.222009\ttraining's amex_metric: 0.798182\tvalid_1's binary_logloss: 0.221884\tvalid_1's amex_metric: 0.798566\n",
      "[1400]\ttraining's binary_logloss: 0.219522\ttraining's amex_metric: 0.798216\tvalid_1's binary_logloss: 0.219327\tvalid_1's amex_metric: 0.798673\n",
      "[1500]\ttraining's binary_logloss: 0.218655\ttraining's amex_metric: 0.798269\tvalid_1's binary_logloss: 0.218462\tvalid_1's amex_metric: 0.798681\n",
      "[1600]\ttraining's binary_logloss: 0.216493\ttraining's amex_metric: 0.798227\tvalid_1's binary_logloss: 0.216197\tvalid_1's amex_metric: 0.798476\n",
      "[1700]\ttraining's binary_logloss: 0.215853\ttraining's amex_metric: 0.798217\tvalid_1's binary_logloss: 0.21554\tvalid_1's amex_metric: 0.798599\n",
      "[1800]\ttraining's binary_logloss: 0.215705\ttraining's amex_metric: 0.798244\tvalid_1's binary_logloss: 0.21541\tvalid_1's amex_metric: 0.798637\n",
      "[1900]\ttraining's binary_logloss: 0.215206\ttraining's amex_metric: 0.798351\tvalid_1's binary_logloss: 0.214891\tvalid_1's amex_metric: 0.79853\n",
      "[2000]\ttraining's binary_logloss: 0.214719\ttraining's amex_metric: 0.798458\tvalid_1's binary_logloss: 0.214374\tvalid_1's amex_metric: 0.798544\n",
      "[2100]\ttraining's binary_logloss: 0.214586\ttraining's amex_metric: 0.798404\tvalid_1's binary_logloss: 0.214252\tvalid_1's amex_metric: 0.798648\n",
      "[2200]\ttraining's binary_logloss: 0.2145\ttraining's amex_metric: 0.79843\tvalid_1's binary_logloss: 0.214175\tvalid_1's amex_metric: 0.798542\n",
      "[2300]\ttraining's binary_logloss: 0.214258\ttraining's amex_metric: 0.798438\tvalid_1's binary_logloss: 0.213901\tvalid_1's amex_metric: 0.798651\n",
      "[2400]\ttraining's binary_logloss: 0.214215\ttraining's amex_metric: 0.798478\tvalid_1's binary_logloss: 0.213858\tvalid_1's amex_metric: 0.798421\n",
      "[2500]\ttraining's binary_logloss: 0.214157\ttraining's amex_metric: 0.798421\tvalid_1's binary_logloss: 0.213793\tvalid_1's amex_metric: 0.798425\n",
      "[2600]\ttraining's binary_logloss: 0.214061\ttraining's amex_metric: 0.798484\tvalid_1's binary_logloss: 0.213697\tvalid_1's amex_metric: 0.79834\n",
      "[2700]\ttraining's binary_logloss: 0.213924\ttraining's amex_metric: 0.798514\tvalid_1's binary_logloss: 0.213555\tvalid_1's amex_metric: 0.79832\n",
      "[2800]\ttraining's binary_logloss: 0.213949\ttraining's amex_metric: 0.798572\tvalid_1's binary_logloss: 0.213606\tvalid_1's amex_metric: 0.798591\n",
      "[2900]\ttraining's binary_logloss: 0.213825\ttraining's amex_metric: 0.798595\tvalid_1's binary_logloss: 0.213464\tvalid_1's amex_metric: 0.798514\n",
      "[3000]\ttraining's binary_logloss: 0.213708\ttraining's amex_metric: 0.79858\tvalid_1's binary_logloss: 0.213329\tvalid_1's amex_metric: 0.798368\n",
      "------------fold 4 CV score is 0.7983679035310003--------------\n",
      "Out of folds CV score is 0.79838162990107\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Train & Evaluate\n",
    "# ====================================================\n",
    "def train_and_evaluate(train, test):\n",
    "\n",
    "    # Get feature list\n",
    "    features = [col for col in train.columns if col not in ['customer_ID', CFG.target]]\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': CFG.metric,\n",
    "        'boosting': CFG.boosting_type,\n",
    "        'seed': CFG.seed,\n",
    "        'max_depth': 3,\n",
    "        'num_leaves': 15,\n",
    "        'learning_rate': 0.01,\n",
    "        #'feature_fraction': 0.20,\n",
    "        'bagging_freq': 10,\n",
    "        'bagging_fraction': 0.5,\n",
    "        'n_jobs': -1,\n",
    "        'lambda_l2': 2,\n",
    "        'path_smooth': 20, #added new\n",
    "        'min_data_in_leaf': 40, #changed from 40\n",
    "        'force_col_wise': True\n",
    "        #'device': 'gpu',\n",
    "        #'max_bin': 255,\n",
    "        #'gpu_platform_id': 1,\n",
    "        #'gpu_device_id': 0\n",
    "        }\n",
    "    # Create a numpy array to store test predictions\n",
    "    test_predictions = np.zeros(len(test))\n",
    "    # Create a numpy array to store out of folds predictions\n",
    "    oof_predictions = np.zeros(len(train))\n",
    "    kfold = StratifiedKFold(n_splits = CFG.n_folds, shuffle = True, random_state = CFG.seed)\n",
    "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train, train[CFG.target])):\n",
    "        print(' ')\n",
    "        print('-'*50)\n",
    "        print(f'Training fold {fold} with {len(features)} features...')\n",
    "        x_train, x_val = train[features].iloc[trn_ind], train[features].iloc[val_ind]\n",
    "        y_train, y_val = train[CFG.target].iloc[trn_ind], train[CFG.target].iloc[val_ind]\n",
    "        lgb_train = lgb.Dataset(x_train, y_train)\n",
    "        lgb_valid = lgb.Dataset(x_val, y_val)\n",
    "\n",
    "        model = lgb.train(\n",
    "            params = params,\n",
    "            train_set = lgb_train,\n",
    "            num_boost_round = 3000,\n",
    "            valid_sets = [lgb_train, lgb_valid],\n",
    "            #early_stopping_rounds = 500,\n",
    "            verbose_eval = 100,\n",
    "            feval = lgb_amex_metric)\n",
    "        # Save best model\n",
    "        joblib.dump(model, OUTPUT_DIR+f'stacking_lgbm_fold{fold}.pkl')\n",
    "        # Predict validation\n",
    "        val_pred = model.predict(x_val)\n",
    "        # Add to out of folds array\n",
    "        oof_predictions[val_ind] = val_pred\n",
    "        # Predict the test set\n",
    "        test_pred = model.predict(test[features])\n",
    "        test_predictions += test_pred / CFG.n_folds\n",
    "        # Compute fold metric\n",
    "        score = amex_metric(y_val, val_pred)\n",
    "        print(f'------------fold {fold} CV score is {score}--------------')\n",
    "        del x_train, x_val, y_train, y_val, lgb_train, lgb_valid\n",
    "        gc.collect()\n",
    "    # Compute out of folds metric\n",
    "    score = amex_metric(train[CFG.target], oof_predictions)\n",
    "    print(f'Out of folds CV score is {score}')\n",
    "    # Create a dataframe to store out of folds predictions\n",
    "    oof_df = pd.DataFrame({'customer_ID': train['customer_ID'], 'target': train[CFG.target], 'prediction': oof_predictions})\n",
    "    oof_df.to_csv(OUTPUT_DIR+f'oof_{expt_name}_seed{CFG.seed}.csv', index = False)\n",
    "    # Create a dataframe to store test prediction\n",
    "    test_df = pd.DataFrame({'customer_ID': test['customer_ID'], 'prediction': test_predictions})\n",
    "    test_df.to_csv(OUTPUT_DIR+f'test_{expt_name}_seed{CFG.seed}.csv', index = False)\n",
    "\n",
    "\n",
    "seed_everything(CFG.seed)\n",
    "#train, test = read_data()\n",
    "train_and_evaluate(train, test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "                                         customer_ID  target  prediction\n0  0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...       0    0.000839\n1  00000fd6641609c6ece5454664794f0340ad84dddce9a2...       0    0.001459\n2  00001b22f846c82c51f6e3958ccd81970162bae8b007e8...       0    0.001751\n3  000041bdba6ecadd89a52d11886e8eaaec9325906c9723...       0    0.004509\n4  00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8a...       0    0.001423",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>customer_ID</th>\n      <th>target</th>\n      <th>prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...</td>\n      <td>0</td>\n      <td>0.000839</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>00000fd6641609c6ece5454664794f0340ad84dddce9a2...</td>\n      <td>0</td>\n      <td>0.001459</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00001b22f846c82c51f6e3958ccd81970162bae8b007e8...</td>\n      <td>0</td>\n      <td>0.001751</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>000041bdba6ecadd89a52d11886e8eaaec9325906c9723...</td>\n      <td>0</td>\n      <td>0.004509</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8a...</td>\n      <td>0</td>\n      <td>0.001423</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacking = pd.read_csv('../stacking_lgbm_6/oof_stacking_lgbm_6_seed42.csv')\n",
    "stacking.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "0.79838162990107"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amex_metric(stacking.target, stacking.prediction)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "stacking_6 = pd.read_csv('../stacking_lgbm_6/test_stacking_lgbm_6_seed42.csv')\n",
    "stacking_5 = pd.read_csv('../stacking_lgbm_5/test_stacking_lgbm_5_seed42.csv')\n",
    "df = pd.read_csv('../stacking_lgbm_5/submission.csv')\n",
    "df_clipped = pd.read_csv('../stacking_lgbm_5/submission_clipped.csv')\n",
    "best = pd.read_csv('../stacking_lgbm_5/submission_best.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "                                         customer_ID  prediction\n0  00000469ba478561f23a92a868bd366de6f6527a684c9a...    0.014540\n1  00001bf2e77ff879fab36aa4fac689b9ba411dae63ae39...    0.000340\n2  0000210045da4f81e5f122c6bde5c2a617d03eef67f82c...    0.026672\n3  00003b41e58ede33b8daf61ab56d9952f17c9ad1c3976c...    0.193853\n4  00004b22eaeeeb0ec976890c1d9bfc14fd9427e98c4ee9...    0.853768",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>customer_ID</th>\n      <th>prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00000469ba478561f23a92a868bd366de6f6527a684c9a...</td>\n      <td>0.014540</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>00001bf2e77ff879fab36aa4fac689b9ba411dae63ae39...</td>\n      <td>0.000340</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0000210045da4f81e5f122c6bde5c2a617d03eef67f82c...</td>\n      <td>0.026672</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00003b41e58ede33b8daf61ab56d9952f17c9ad1c3976c...</td>\n      <td>0.193853</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00004b22eaeeeb0ec976890c1d9bfc14fd9427e98c4ee9...</td>\n      <td>0.853768</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clipped.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "KstestResult(statistic=0.04997074477001928, pvalue=0.0)"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ks_2samp(stacking_6.prediction, stacking_5.prediction)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "KstestResult(statistic=0.5417116851120621, pvalue=0.0)"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ks_2samp(stacking.prediction, best.prediction)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 720x720 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.hist(best.prediction, bins = 100, alpha=0.5, label='current best')\n",
    "plt.hist(df.prediction, bins = 100, alpha=0.5, label='new public code')\n",
    "plt.hist(df_clipped.prediction, bins = 100, alpha=0.5, label='clipped')\n",
    "plt.hist(test_df.prediction, bins = 100, alpha=0.5, label='stacked')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ax1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [22]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43max1\u001B[49m\u001B[38;5;241m.\u001B[39mplot(x, y)\n\u001B[0;32m      2\u001B[0m ax1\u001B[38;5;241m.\u001B[39mset_title(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSharing Y axis\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      3\u001B[0m ax2\u001B[38;5;241m.\u001B[39mscatter(x, y)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'ax1' is not defined"
     ]
    }
   ],
   "source": [
    "ax1.plot(x, y)\n",
    "ax1.set_title('Sharing Y axis')\n",
    "ax2.scatter(x, y)\n",
    "plt.hist(best.prediction, bins=100)\n",
    "plt.title('Test Predictions')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}